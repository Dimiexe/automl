{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import arff\n",
    "import re\n",
    "import codecs\n",
    "path_to_dataset_folder = '../DATAsets'\n",
    "path_to_ready_csv = path_to_dataset_folder + '/DATA/CSVs'\n",
    "path_encoded_csv = path_to_dataset_folder + '/DATA/CSVs_reverse_encoded'\n",
    "target_test_path = path_to_dataset_folder + '/DATA/Target_feature_test_CSVs'\n",
    "first_nan_treat_path = path_to_dataset_folder + '/DATA/1st_NaN_Treatment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array_split(df,2)\n",
    "online_retail_II1=x[0]\n",
    "online_retail_II2=x[1]\n",
    "online_retail_II1.to_csv(first_nan_treat_path + '/online_retail_II1.csv', index = False)\n",
    "online_retail_II2.to_csv(first_nan_treat_path + '/online_retail_II2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Train_AccountInfo%BaseCharges.csv'\n",
    "df = pd.read_csv(path_to_dataset_folder + '/DATA/Target_feature_test_CSVs' + '/' + file,header = 0, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = df.dtypes\n",
    "cardi = df.apply(pd.Series.nunique)\n",
    "df2=pd.DataFrame({'Types': types,\n",
    "                  'Cardinality': cardi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Types</th>\n",
       "      <th>Cardinality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TotalCharges</th>\n",
       "      <td>float64</td>\n",
       "      <td>4986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOE</th>\n",
       "      <td>object</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElectronicBilling</th>\n",
       "      <td>object</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ContractType</th>\n",
       "      <td>object</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaymentMethod</th>\n",
       "      <td>object</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>float64</td>\n",
       "      <td>1470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Types  Cardinality\n",
       "TotalCharges       float64         4986\n",
       "DOE                 object           72\n",
       "ElectronicBilling   object            2\n",
       "ContractType        object            3\n",
       "PaymentMethod       object            4\n",
       "Target             float64         1470"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to extract the year, month, day of the week, day of the year, hour in decimal form \n",
    "#(e.g. 12:30 -> 12.5, 13:15 -> 13.25) and number of day since observation begun.\n",
    "col_name = 'date'\n",
    "year = []\n",
    "month = []\n",
    "wday = []\n",
    "yday = []\n",
    "hour = []\n",
    "tday = []\n",
    "for i in range(df.shape[0]):\n",
    "    d=parse(df.loc[i,col_name])\n",
    "    year.append(d.timetuple().tm_year)\n",
    "    month.append(d.timetuple().tm_mon)\n",
    "    wday.append(d.timetuple().tm_wday)\n",
    "    yday.append(d.timetuple().tm_yday)\n",
    "    hour.append(d.timetuple().tm_hour + d.timetuple().tm_min/60)\n",
    "    tday.append(i+1)\n",
    "df=df.assign(year=year,\n",
    "             #month=month,\n",
    "             wday=wday,\n",
    "             yday=yday,\n",
    "             #hour=hour,\n",
    "             tday=tday)\n",
    "df.drop(col_name,axis=1,inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is used to randomly chose the target variable from all float type features, in all datasets\n",
    "#contained in \"path\" dir excluding the ones with their name given as an exclusion in \"exclusions\",\n",
    "#and move it in the last column of the dataset under the feature name \"Target\". The new files are\n",
    "#saved in the \"Target_feature_test_CSVs\" folder of the \"../DATAsets/DATA\" dir with the feature\n",
    "#selected as target, added in the back of the original file name. For manual target feature\n",
    "#assignment the \"target_features\" can be a list containing the names of the desired features.\n",
    "def random_targets(path,exclusions = None, target_features = None):\n",
    "    files = os.listdir(path)\n",
    "    for file in np.setdiff1d(files, exclusions,assume_unique=True):\n",
    "        df = pd.read_table(path + '/' + file, header = 0, sep = ',')\n",
    "        #Create the list containing the candidate features for being targets.\n",
    "        elig_target = []\n",
    "        for i,feat in enumerate(df.dtypes):\n",
    "            #Check if it's float and not constant.\n",
    "            if feat == 'float64' and len(df.iloc[:,i].unique()) > 1:\n",
    "                elig_target.append(df.columns.values[i])\n",
    "        if target_features != None:\n",
    "            elig_target = target_features\n",
    "        if len(elig_target)>0:\n",
    "            \n",
    "            #Set how many datasets will be produced based on the number of features.\n",
    "            iterations = 1\n",
    "            if i+1 >= 15 and i+1 < 24:\n",
    "                iterations = 2\n",
    "            elif i+1 >= 24 and i+1 < 32:\n",
    "                iterations = 3\n",
    "            elif i+1 >= 32 and i+1 < 40:\n",
    "                iterations = 4\n",
    "            elif i+1 >= 40:\n",
    "                iterations = 5\n",
    "                \n",
    "            #Create The new datasets\n",
    "            already_used = []\n",
    "            for iters in range(min(iterations,len(elig_target))):\n",
    "                if iters == 0:  #If this is the 1st target for a particular dataset.\n",
    "                    tar_int = np.random.randint(len(elig_target))\n",
    "                    target_var = elig_target[tar_int]\n",
    "                    already_used.append(target_var)\n",
    "                else: #If this is the 2nd or 3rd target for a particular dataset.\n",
    "                    #Reload untouched dataset if more targets are needed.\n",
    "                    df = pd.read_table(path + '/' + file, header = 0, sep = ',')\n",
    "                    while target_var in already_used:\n",
    "                        tar_int = np.random.randint(len(elig_target))\n",
    "                        target_var = elig_target[tar_int]\n",
    "                    already_used.append(target_var)\n",
    "                target_values = df.loc[:,target_var]\n",
    "                df=df.assign(Target = target_values)\n",
    "                df.drop(target_var,axis=1,inplace=True)\n",
    "                print(file.split('.')[0] + ' --> \"' + target_var + '\"')\n",
    "                invalid_name_chars = [':','/','<','>','*','|','\"','/']\n",
    "                temp_target_var = target_var\n",
    "                for char in invalid_name_chars:\n",
    "                    temp_target_var = temp_target_var.replace(char,'-')\n",
    "                df.to_csv(target_test_path + '/' + file.split('.')[0] + '%' + temp_target_var +\n",
    "                          '.csv',index= False)\n",
    "                \n",
    "        else:\n",
    "            print('File: ' + file + \" couldn't have a target variable assigned!\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_targets(first_nan_treat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for manual dataset generation for specific feature(s) as target(s) or for re-generating\n",
    "#datasets from some SELECTED csv files.\n",
    "#Move the datasets' csv in folder \"manual_targets\" and then specify the names of the features\n",
    "#to be assigned as the new targets in the following manner; \n",
    "#random_targets(manual_path, target_features = ['foo','fee',...]).\n",
    "\n",
    "manual_path = path_to_dataset_folder + '/manual_targets'\n",
    "random_targets(manual_path, target_features = ['13:Meteo_Exterior_Crepusculo','17:Meteo_Exterior_Sol_Sud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesWithNan(df):\n",
    "    nansPerLine = df.isna().sum(axis=1)\n",
    "    lines_with_nan = 0\n",
    "    for l in nansPerLine:\n",
    "        if l>0:\n",
    "            lines_with_nan +=1\n",
    "    return lines_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Used to perform the 1st NaN treatment of the datasets. Any Feature with more than 20% missing\n",
    "#values is discarded. After all such features have been discarded if the lines with at least one\n",
    "#missing vallue are less than 10% of the total remaining lines, they are droped. The resulting\n",
    "#datasets are saved in \"1st_NaN_Treatment\" folder of the \"../DATAsets/DATA\" dir.\n",
    "def first_nan_treatment(path,exclusions = None, feature_threshold = 0.20,\n",
    "                        lines_with_nan_threshold = 0.1, visual_info = False):\n",
    "    files = os.listdir(path)\n",
    "    for f in np.setdiff1d(files, exclusions,assume_unique=True):\n",
    "        df = pd.read_table(path + '/' + f, header = 0, sep = ',')\n",
    "        lines = df.shape[0]\n",
    "        features = df.shape[1]\n",
    "                \n",
    "        #Nans on Feature basis\n",
    "        nanPerFeature = df.isna().sum(axis=0)\n",
    "        indecies_to_drop = []\n",
    "        for i,npf in enumerate(nanPerFeature):\n",
    "            if npf / lines >= feature_threshold:\n",
    "                indecies_to_drop.append(i)\n",
    "        df.drop(df.columns[indecies_to_drop],axis=1,inplace=True)\n",
    "        \n",
    "        #Nans on Line basis\n",
    "        nansPerLine = df.isna().sum(axis=1)\n",
    "        lines_with_nan = linesWithNan(df)\n",
    "        if (lines_with_nan / lines) <= lines_with_nan_threshold:\n",
    "            df.dropna(axis = 0, how='any',inplace=True)\n",
    "        new_lines_with_nan = linesWithNan(df)\n",
    "        if visual_info:\n",
    "            if features != df.shape[1] or lines != df.shape[0] or new_lines_with_nan/df.shape[0] >=lines_with_nan_threshold:\n",
    "                print('File:' + f)\n",
    "                print('Lines:' + str(lines) + ' --> ' +str(df.shape[0]))\n",
    "                print('Features: ' +str(features) + ' --> '+str(df.shape[1]))\n",
    "                print('Lines with NaNs: ' + str(lines_with_nan) + ' ('+\n",
    "                      str(round(lines_with_nan / lines * 10000)/100) + '%) --> ' +\n",
    "                      str(new_lines_with_nan)+ '('+ str(round(new_lines_with_nan /\n",
    "                                                              df.shape[0] * 10000)/100) + '%)')\n",
    "                print('NaNs per Feature:\\n' + str(df.isna().sum(axis=0)))\n",
    "                print('---------------------------------------------')\n",
    "        df.to_csv(first_nan_treat_path + '/' + f,index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_nan_treatment(path_encoded_csv, visual_info=True)\n",
    "unusedFiles = ['BlogFeedbackData_train.csv','core_dataset.csv','household_power_consumption.csv',\n",
    "              'incident_event_log.csv','job-prestige.csv','Train_Demographics.csv']\n",
    "exclusions = os.listdir(path_encoded_csv)\n",
    "exclusions.extend(unusedFiles)\n",
    "first_nan_treatment(path_to_ready_csv,exclusions,visual_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check all datasets after 1st nan treatment and drop any constant columns,\n",
    "#while preserving the naming of the remaining columns.\n",
    "for file in os.listdir(first_nan_treat_path):\n",
    "    df = pd.read_table(first_nan_treat_path + '/'+ file,sep=',')\n",
    "    L = df.shape[1]\n",
    "    drops = []\n",
    "    i=0\n",
    "    for feat in df.columns:\n",
    "        if len(df.loc[:,feat].unique()) == 1:\n",
    "            if i == 0:\n",
    "                print('Processing file: ' + file)\n",
    "                print(df.head())\n",
    "            print('Found one! It is column: ' + feat)\n",
    "            print('Unique value = ', df.loc[0,feat])\n",
    "            drops.append(feat)\n",
    "            L -=1\n",
    "            i+=1\n",
    "    flag = False\n",
    "    if df.columns[0] == '0':\n",
    "        flag = True\n",
    "    df.drop(drops, axis = 1, inplace = True)\n",
    "    if flag:\n",
    "        df.columns = range(L)\n",
    "    if len(drops)>0:\n",
    "        print(df.head())\n",
    "    df.to_csv(first_nan_treat_path + '/'+ file,sep=',',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Used for printing NaN info for the datasets contained in dir \"first_nan_treat_path\"\n",
    "for f in os.listdir(target_test_path):\n",
    "    df =  pd.read_table(target_test_path + '/'+ f,sep=',')\n",
    "    lines = df.shape[0]\n",
    "    nans = df.isnull().sum(axis=1)\n",
    "    nanPerFeature = df.isna().sum(axis=0)\n",
    "    nanFeatureFlag = None\n",
    "    indecies = []\n",
    "    for i,npf in enumerate(nanPerFeature):\n",
    "        if npf / lines !=0:\n",
    "            nanFeatureFlag = True\n",
    "            indecies.append(i)\n",
    "    lines_with_nan = 0\n",
    "    for l in nans:\n",
    "        if l>0:\n",
    "            lines_with_nan +=1\n",
    "    if nanFeatureFlag:\n",
    "        print('File:' + f)\n",
    "        print('Lines:' + str(df.shape[0]))\n",
    "        print('Features: ' +str(df.shape[1]))\n",
    "        print('Lines with NaNs: ' + str(lines_with_nan) + ' ('+ \n",
    "              str(round(lines_with_nan / lines * 10000)/100) + '%)')\n",
    "        print('NaNs per Feature:\\n ^^^^^^^^^^^^^\\n' + str(nanPerFeature[indecies])+\n",
    "             '\\n^^^percentages^^^')\n",
    "        for i in indecies:\n",
    "            print(str(round(nanPerFeature[i]/lines*10000)/100)+'%')\n",
    "        print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Used to drop all lines in every dataset in dir '/DATAsets/DATA/Target_feature_test_CSVs',\n",
    "#with NaN in the response variable.\n",
    "for file in os.listdir(target_test_path):\n",
    "    df = pd.read_table(target_test_path + '/' + file, header = 0, sep = ',')\n",
    "    if df[df.iloc[:,-1].isnull()].index.values != []:\n",
    "        print(file)\n",
    "        print(df[df.iloc[:,-1].isnull()].index.values)\n",
    "        df.drop(df[df.iloc[:,-1].isnull()].index, axis = 0, inplace = True)\n",
    "        df.reset_index(inplace= True)\n",
    "        df.to_csv(target_test_path + '/' + file,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h2o\n",
    "master_path = 'C:/Users/Dimiexe/Desktop'\n",
    "target_test_path = master_path + '/DATAsets/DATA/Target_feature_test_CSVs'\n",
    "ground_truth_path = master_path + '/DATAsets/DATA/ground_truth'\n",
    "all_partials_path = master_path + '/from_helper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h2o.init(max_mem_size = 64, nthreads = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_gTruth_oneByOne(file, source, gTrouth_path, ntrees, depth):\n",
    "    print('processing file: ' + file + ' | ntrees:' + str(ntrees) + ', Tree depth:' + str(depth))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time() #Start counting training time\n",
    "    \n",
    "    model = h2o.estimators.H2ORandomForestEstimator(model_id = 'RFesti',\n",
    "                                                    seed = 888,\n",
    "                                                    ntrees = ntrees,\n",
    "                                                    max_depth = depth,\n",
    "                                                    nfolds = 10,\n",
    "                                                    keep_cross_validation_models = False)\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time #Stop timing\n",
    "    \n",
    "    params = h2o.H2OFrame({'ntrees':ntrees,\n",
    "                           'max_depth':depth,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, gTrouth_path + '/RF_one_by_one/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_ntr' + str(ntrees) + '_dpth' + str(depth)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLM_gTruth_oneByOne(file, source, gTrouth_path, alpha):\n",
    "    print('processing file: ' + file + ' | alpha:' + str(alpha))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "           \n",
    "    model = h2o.estimators.glm.H2OGeneralizedLinearEstimator(model_id = 'GLMesti',\n",
    "                                                             seed = 888,\n",
    "                                                             alpha = alpha,\n",
    "                                                             lambda_search = True,\n",
    "                                                             nlambdas = 100,\n",
    "                                                             standardize = True,\n",
    "                                                             nfolds = 10,\n",
    "                                                             keep_cross_validation_models = False)\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time\n",
    "    \n",
    "    lambda_param = float(model.summary()['regularization'][0].split('= ')[-1].split(' ')[0])\n",
    "    params = h2o.H2OFrame({'alpha':alpha,\n",
    "                           'lambda':lambda_param,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, gTrouth_path + '/GLM_one_by_one/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_alpha' + str(alpha) + '_lambda' + str(lambda_param) +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGboost_gTruth_oneByOne(file, source, gTrouth_path, ntrees, depth, colsample_bytree, method = 'cpu'):\n",
    "    print('processing file: ' + file + ' | ntrees:' + str(ntrees) + ', Tree depth:' + str(depth)+\n",
    "         ', colsPerTree:' + str(colsample_bytree))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = h2o.estimators.xgboost.H2OXGBoostEstimator(model_id = 'XGesti',\n",
    "                                                       seed = 888,\n",
    "                                                       backend= method,\n",
    "                                                       ntrees = ntrees,\n",
    "                                                       max_depth = depth,\n",
    "                                                       col_sample_rate_per_tree = colsample_bytree,\n",
    "                                                       nfolds = 10,\n",
    "                                                       keep_cross_validation_models = False)\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time\n",
    "    \n",
    "    params = h2o.H2OFrame({'ntrees':ntrees,\n",
    "                           'max_depth':depth,\n",
    "                           'cols_per_tree': colsample_bytree,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, gTrouth_path + '/XGboost_one_by_one/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_ntr' + str(ntrees) + '_dpth' + str(depth) + '_colsPerTree' + str(colsample_bytree) + '.csv')\n",
    "    h2o.download_csv(params, all_partials_path + '/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_ntr' + str(ntrees) + '_dpth' + str(depth) + '_colsPerTree' + str(colsample_bytree) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a dataFrame containing different RF hyperparameter combinations and a set of\n",
    "#indices pointing to some lines in that dataFrame, this function returns the one index\n",
    "#that points to that hyperparameter combination which minimises the size of the RF. \n",
    "def find_eco_index(indices, data):\n",
    "    for i,ind in enumerate(indices):\n",
    "        if i == 0:\n",
    "            temp = data.loc[ind,'ntrees'] * data.loc[ind,'max_depth']\n",
    "            eco_index = ind\n",
    "        else:\n",
    "            if temp > data.loc[ind,'ntrees'] * data.loc[ind,'max_depth']:\n",
    "                temp = data.loc[ind,'ntrees'] * data.loc[ind,'max_depth']\n",
    "                eco_index = ind\n",
    "    return eco_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results_RF(source_path, visualInfo = False):\n",
    "    files = os.listdir(source_path)\n",
    "    for i,f in enumerate(files):\n",
    "        if i == 0:\n",
    "            data = pd.read_table(os.path.join(source_path, f),sep=',')\n",
    "        else:\n",
    "            data = pd.concat([data,\n",
    "                              pd.read_table(os.path.join(source_path, f),sep=',')\n",
    "                             ], ignore_index=True)\n",
    "    if visualInfo:\n",
    "        print(data.head(10))\n",
    "    best_rmse = data['rmse'].min()\n",
    "    best_index = data[data['rmse'] == best_rmse].index\n",
    "    #The below line guarantees that, even if there are more than one hyperparameter\n",
    "    #combinations that give the same-minimum rmse, the smallest RF is chosen.\n",
    "    best_index = find_eco_index(best_index, data)\n",
    "    \n",
    "    #Below every hyperparameter combination with an rmse 1% higher than the best rmse\n",
    "    #is considered, and the one resulting to the smallest RF is chosen.\n",
    "    indices = data[data['rmse'] <= best_rmse * 1.01].index\n",
    "    eco_index = find_eco_index(indices, data)\n",
    "    \n",
    "    df = pd.DataFrame({'ntrees_best':[data.loc[best_index,'ntrees']],\n",
    "                       'max_depth_best':[data.loc[best_index,'max_depth']],\n",
    "                       'rmse_best':[data.loc[best_index,'rmse']],\n",
    "                       'ntrees_eco':[data.loc[eco_index,'ntrees']],\n",
    "                       'max_depth_eco':[data.loc[eco_index,'max_depth']],\n",
    "                       'rmse_eco':[data.loc[eco_index,'rmse']],\n",
    "                       'calc_time':[data['calc_time'].sum()]\n",
    "                      })\n",
    "    df.to_csv(source_path + '/../RF/'+ f.split('_ntr')[0] + '.csv',index= False)\n",
    "    for f in files:\n",
    "        os.remove(source_path + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results_GLM(source_path, visualInfo = False):\n",
    "    files = os.listdir(source_path)\n",
    "    for i,f in enumerate(files):\n",
    "        if i == 0:\n",
    "            data = pd.read_table(os.path.join(source_path, f), header = 0, sep = ',')\n",
    "        else:\n",
    "            data = pd.concat([data,\n",
    "                              pd.read_table(os.path.join(source_path, f), header = 0, sep = ',')\n",
    "                             ], ignore_index=True)\n",
    "    if visualInfo:\n",
    "        print(data.head(10))\n",
    "    best_rmse = data['rmse'].min()\n",
    "    best_index = data[data['rmse'] == best_rmse].index\n",
    "    if len(best_index.values) > 1: #Deals with multiple equally good indices\n",
    "        temp = 888\n",
    "        for l in best_index.values:\n",
    "            if abs(4.5 - l) <= temp:\n",
    "                temp = abs(4.5 - l)\n",
    "                flag = l\n",
    "        best_index = l # \"best_index\" contains a number now.\n",
    "    else:\n",
    "        #This else is needed because \"best_index\" contains an array of length 1; e.g. [8]\n",
    "        best_index = best_index[0]\n",
    "    print('Best index:' + str(best_index))\n",
    "    df = pd.DataFrame({'alpha_best':[data.loc[best_index,'alpha']],\n",
    "                       'lambda_best':[data.loc[best_index,'lambda']],\n",
    "                       'rmse_best':[data.loc[best_index,'rmse']],\n",
    "                       'calc_time':[data['calc_time'].sum()]\n",
    "                      })\n",
    "    df.to_csv(source_path + '/../GLM/'+ f.split('_alpha')[0] + '.csv',index= False)\n",
    "    for f in files:\n",
    "        os.remove(source_path + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results_XGboost(source_path, visualInfo = False):\n",
    "    files = os.listdir(source_path)\n",
    "    for i,f in enumerate(files):\n",
    "        if i == 0:\n",
    "            data = pd.read_table(os.path.join(source_path, f),sep=',')\n",
    "        else:\n",
    "            data = pd.concat([data,\n",
    "                              pd.read_table(os.path.join(source_path, f),sep=',')\n",
    "                             ], ignore_index=True)\n",
    "    if visualInfo:\n",
    "        print(data.head(10))\n",
    "    best_rmse = data['rmse'].min()\n",
    "    best_index = data[data['rmse'] == best_rmse].index\n",
    "    #The below line guarantees that, even if there are more than one hyperparameter\n",
    "    #combinations that give the same-minimum rmse, the smallest RF is chosen.\n",
    "    best_index = find_eco_index(best_index, data)\n",
    "    \n",
    "    #Below every hyperparameter combination with an rmse 1% higher than the best rmse\n",
    "    #is considered, and the one resulting to the smallest RF size is chosen.\n",
    "    indices = data[data['rmse'] <= best_rmse * 1.01].index\n",
    "    eco_index = find_eco_index(indices, data)\n",
    "    \n",
    "    df = pd.DataFrame({'ntrees_best':[data.loc[best_index,'ntrees']],\n",
    "                       'max_depth_best':[data.loc[best_index,'max_depth']],\n",
    "                       'cols_per_tree_best':[data.loc[best_index,'cols_per_tree']],\n",
    "                       'rmse_best':[data.loc[best_index,'rmse']],\n",
    "                       'ntrees_eco':[data.loc[eco_index,'ntrees']],\n",
    "                       'max_depth_eco':[data.loc[eco_index,'max_depth']],\n",
    "                       'cols_per_tree_eco':[data.loc[eco_index,'cols_per_tree']],\n",
    "                       'rmse_eco':[data.loc[eco_index,'rmse']],\n",
    "                       'calc_time':[data['calc_time'].sum()]\n",
    "                      })\n",
    "    df.to_csv(source_path + '/../XGboost/'+ f.split('_ntr')[0] + '.csv',index= False)\n",
    "    for f in files:\n",
    "        os.remove(source_path + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "def help_combine(file_name, source, destination, algo):\n",
    "    files = os.listdir(source)\n",
    "    for f in files:\n",
    "        if algo in ['XGBoost', 'RF']:\n",
    "            if f.split('_ntr')[0] == '.'.join(file_name.split('.')[0:-1]):\n",
    "                #Copy f to destination (==Algo_one_by_one folder)\n",
    "                copyfile(source + '/' + f, destination + '/' + f)\n",
    "        elif algo == 'GLM':\n",
    "            if f.split('_alpha')[0] == '.'.join(file_name.split('.')[0:-1]):\n",
    "                #Copy f to destination (==Algo_one_by_one folder)\n",
    "                copyfile(source + '/' + f, destination + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Manually implemented Random Forest\n",
    "param_ntrees = [25,50,75,100,200,300,400,500]\n",
    "param_maxDepth = [20,40,60,80]\n",
    "extra_ignore = []\n",
    "alreadyDone = os.listdir(ground_truth_path + '/RF')\n",
    "datasets = os.listdir(target_test_path)\n",
    "for f in np.sort(np.setdiff1d(datasets, alreadyDone + extra_ignore, assume_unique=True)):\n",
    "    for nt in param_ntrees:\n",
    "        for dpth in param_maxDepth:\n",
    "            h2o.remove_all()\n",
    "            h2o.remove_all()\n",
    "            #The below variable \"file_name\" and the subsequent if statement are used to\n",
    "            #determine if a combination has already been tested, and thus be skiped.\n",
    "            file_name = '.'.join(f.split('.')[0:-1]) + '_ntr' + str(nt) + '_dpth' + str(dpth) + '.csv'\n",
    "            if not os.path.isfile(ground_truth_path + '/RF_one_by_one/' + file_name):\n",
    "                RF_gTruth_oneByOne(f, target_test_path, ground_truth_path,nt,dpth)\n",
    "            h2o.remove_all()\n",
    "    combine_results_RF(ground_truth_path + '/RF_one_by_one', visualInfo = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Manually implemented Generalised Linear Model\n",
    "y = range(0, 1100, 125)\n",
    "param_alpha = [x / 1000 for x in y]\n",
    "#param_lambda = being searched internally for the best value of lambda for each value of alpha by search_lambda = True\n",
    "\n",
    "alreadyDone = os.listdir(ground_truth_path + '/GLM')\n",
    "datasets = os.listdir(target_test_path)\n",
    "for f in np.setdiff1d(datasets, alreadyDone, assume_unique=True):\n",
    "    start_time = time.time()\n",
    "    for A in param_alpha:\n",
    "        h2o.remove_all()\n",
    "        h2o.remove_all()\n",
    "        GLM_gTruth_oneByOne(f , target_test_path, ground_truth_path,A)\n",
    "        h2o.remove_all()\n",
    "    combine_results_GLM(ground_truth_path + '/GLM_one_by_one', visualInfo = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Manually implimented XGboost\n",
    "param_ntrees = [25,50,100,200]\n",
    "param_maxDepth = [6,10,15]\n",
    "colsample_bytree = [.6,.7,.8,.9]\n",
    "cpu_gpu_pick = 20971520  #Size of file, in bytes, over which the gpu method is picked. 20MB\n",
    "alreadyDone = os.listdir(ground_truth_path + '/XGboost')\n",
    "extra_ignore = ['processed_Houston%humidity.csv']\n",
    "datasets = os.listdir(target_test_path)\n",
    "for f in np.sort(np.setdiff1d(datasets, alreadyDone + extra_ignore, assume_unique=True)):\n",
    "    f_size = os.path.getsize(target_test_path + '/' + f)\n",
    "    print(f_size)\n",
    "    help_combine(f, all_partials_path, ground_truth_path + '/XGboost_one_by_one/', algo = 'XGBoost')\n",
    "    for nt in param_ntrees:\n",
    "        for dpth in param_maxDepth:\n",
    "            for colsPerTree in colsample_bytree:\n",
    "                h2o.remove_all()\n",
    "                h2o.remove_all()\n",
    "                file_name = '.'.join(f.split('.')[0:-1]) + '_ntr' + str(nt) + '_dpth' + str(dpth) + '_colsPerTree' + str(colsPerTree) + '.csv'\n",
    "                if not os.path.isfile(ground_truth_path + '/XGboost_one_by_one/' + file_name):\n",
    "                    if f_size < cpu_gpu_pick or dpth > 16:\n",
    "                        XGboost_gTruth_oneByOne(f , target_test_path, ground_truth_path,\n",
    "                                                nt, dpth, colsPerTree, 'cpu')\n",
    "                    else:\n",
    "                        XGboost_gTruth_oneByOne(f , target_test_path, ground_truth_path,\n",
    "                                                nt, dpth, colsPerTree, 'gpu')\n",
    "                h2o.remove_all()\n",
    "    combine_results_XGboost(ground_truth_path + '/XGboost_one_by_one', visualInfo = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.remove_all()\n",
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import arff\n",
    "import re\n",
    "import codecs\n",
    "path_to_dataset_folder = '../DATAsets'\n",
    "path_to_ready_csv = path_to_dataset_folder + '/DATA/CSVs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV:0\n",
      "txt:0\n",
      "excel:0\n",
      "data:0\n",
      "arff:0\n"
     ]
    }
   ],
   "source": [
    "#Display counts of processable files in dataset folder\n",
    "txt_count=0\n",
    "xlsx_count=0\n",
    "csv_count=0\n",
    "data_count=0\n",
    "arff_count=0\n",
    "for filename in os.listdir(path_to_dataset_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        txt_count+=1\n",
    "    elif filename.endswith('.xls') or filename.endswith('.xlsx'):\n",
    "        xlsx_count+=1\n",
    "    elif filename.endswith('.data') or filename.endswith('.dat'):\n",
    "        data_count+=1\n",
    "    elif filename.endswith('.csv'):\n",
    "        csv_count+=1\n",
    "    elif filename.endswith('.arff'):\n",
    "        arff_count+=1\n",
    "print('CSV:' + str(csv_count))\n",
    "print('txt:' + str(txt_count))\n",
    "print('excel:' + str(xlsx_count))\n",
    "print('data:' + str(data_count))\n",
    "print('arff:' + str(arff_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkTargetDir(full_name):\n",
    "    writeDir = ''.join('/'.join(full_name.split('/')[0:-1]) + '/CSVs')\n",
    "    if not os.path.isdir(writeDir):\n",
    "        os.mkdir(writeDir)\n",
    "    return writeDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tabAndWhite(full_name):\n",
    "    print('Dealing with tabs and whitespaces in: ' + full_name.split('/')[-1])\n",
    "    with codecs.open(full_name, 'r', 'utf-8') as curFile:\n",
    "        data = curFile.readlines()\n",
    "    data = [re.sub('\\t', ' ',x.strip()) for x in data]    #Turning tabs to whitespace,stripping start/end.\n",
    "    data = [re.sub(' +',' ',x) for x in data]    #Turning multiple whitespaces to length of one.\n",
    "    data = '\\n'.join(data)\n",
    "    tempFile = ''.join('/'.join(full_name.split('/')[0:-1]) + '/v2_' + full_name.split('/')[-1])\n",
    "    with codecs.open(tempFile, 'w', 'utf-8') as curFile:\n",
    "        for row in data:\n",
    "            curFile.write(row)\n",
    "    return tempFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_txt_data(original_name):\n",
    "    filename = original_name.split('/')[-1]\n",
    "    print('Processing ' + filename)\n",
    "    full_name = tabAndWhite(original_name)\n",
    "    file_size = os.stat(full_name).st_size\n",
    "    with codecs.open(full_name, 'r', 'utf-8') as curFile:\n",
    "        #Dealing with small (if) and bigger size files (else) while detecting header.\n",
    "        if file_size < 4100:\n",
    "            header_present = csv.Sniffer().has_header(''.join(curFile.readlines(file_size - 50)))\n",
    "        else:\n",
    "            header_present = csv.Sniffer().has_header(''.join(curFile.readlines(4096)))\n",
    "        curFile.seek(0)\n",
    "        if header_present:\n",
    "            print('Header present in file: ' + filename)\n",
    "            df = pd.read_table(curFile,header = 0, sep = None)\n",
    "        else:\n",
    "            print('No header in file: ' + filename)\n",
    "            df = pd.read_table(curFile,header = None, sep = None)\n",
    "    os.remove(full_name)\n",
    "    writeDir = checkTargetDir(full_name)\n",
    "    df.to_csv(os.path.join(writeDir,'.'.join(filename.split('.')[0:-1]) +'.csv'),index =False)\n",
    "    #print(df.head())    #For visual confirmation\n",
    "    print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_excel(full_name):\n",
    "    filename = full_name.split('/')[-1]\n",
    "    print('Processing ' + filename)\n",
    "    df = pd.read_excel(full_name)\n",
    "    writeDir = checkTargetDir(full_name)\n",
    "    df.to_csv(os.path.join(writeDir,'.'.join(filename.split('.')[0:-1]) +'.csv'),index =False)\n",
    "    #print(df.head())    #For visual confirmation\n",
    "    print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arff(full_name):\n",
    "    print('Processing ' + full_name.split('/')[-1])\n",
    "    with codecs.open(full_name,encoding = 'utf-8') as curFile:\n",
    "        df = arff.load(curFile)\n",
    "    df = pd.DataFrame(df['data'])\n",
    "    writeDir = checkTargetDir(full_name)\n",
    "    df.to_csv(os.path.join(writeDir,'.'.join(filename.split('.')[0:-1]) +'.csv'),index =False)\n",
    "    #print(df.head())    #For visual confirmation\n",
    "    print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Creating \"files\" as a one time assignment of listdir so as to\n",
    "#avoid processing later-created-temp files as well.\n",
    "files = os.listdir(path_to_dataset_folder)\n",
    "for filename in files:\n",
    "    full_name = os.path.join(path_to_dataset_folder, filename)\n",
    "    suf = filename.split('.')[-1]\n",
    "    if suf in [\"txt\",\"csv\",\"data\",\"dat\"]:\n",
    "        process_csv_txt_data(full_name)\n",
    "    elif suf in [\"xls\", \"xlsx\"]:\n",
    "        process_excel(full_name)\n",
    "    elif suf == \"arff\":\n",
    "        process_arff(full_name)\n",
    "    else:\n",
    "        print('Unsupported file: ' + filename + ' ...ignoring...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Creating City Files of weather attributes form 'Historical Hourly Weather Data'\n",
    "#Files with attributes must be in a subfolder \"Historical_Hourly_Weather_Data_for_processing\" \n",
    "#while the file \"city_attributes.csv\" needs to be on the same dir as the folder \n",
    "#\"Historical_Hourly_Weather_Data_for_processing\"\n",
    "process_path = os.path.join(path_to_dataset_folder + '/Historical_Hourly_Weather_Data_for_processing')\n",
    "attr_list =[]\n",
    "for file in os.listdir(process_path):\n",
    "    attr_list.append(file.split('.')[0])\n",
    "df = pd.read_table(os.path.join(path_to_dataset_folder,'city_attributes.csv'),sep=',')\n",
    "city_names = []\n",
    "for item in df.City:\n",
    "    city_names.append(item)\n",
    "for i in range(len(city_names)):\n",
    "    print('Checking file: ' + city_names[i] +'.csv')\n",
    "    df = pd.DataFrame(columns=attr_list)\n",
    "    for file in os.listdir(process_path):\n",
    "        temp = pd.read_table(os.path.join(process_path, file),sep=',')\n",
    "        df.loc[:,file.split('.')[0]] = temp.loc[:,city_names[i]]\n",
    "    df = df.dropna(how='all')    #Drop lines where all values are NaN.\n",
    "    if df.isnull().sum().sum() < 500:    #Keep the files with less than 500 total NaN values.\n",
    "        df = df.dropna(how='any')    #From the kept files, drop lines with even one NaN.\n",
    "        df.to_csv(path_to_dataset_folder +'/processed_'+city_names[i]+'.csv',index= False)\n",
    "        print('Pass')\n",
    "    else:\n",
    "        print(\"Too many NaN's, ignoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is used to see how many lines of data exist in the processesed CSVs\n",
    "for file in os.listdir(path_to_dataset_folder):\n",
    "    if file.split('.')[-1] == 'csv':\n",
    "        df = pd.read_table(path_to_dataset_folder + '/'+ file,sep=',')\n",
    "        print('File: ' + file)\n",
    "        print(len(df))\n",
    "        print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping lines with even one NaN value in the \"PRSA_Data_20130301-20170228\" files \n",
    "#The folder \"PRSA_Data_20130301-20170228\" needs to be in the \"DATAsets\" dir, and the new\n",
    "#files are generated in the same dir.\n",
    "for file in os.listdir(path_to_dataset_folder + '/PRSA_Data_20130301-20170228):\n",
    "    if file.split('.')[-1] == 'csv':\n",
    "        df = pd.read_table(path_to_dataset_folder + '/'+ file,sep=',')\n",
    "        df = df.dropna(how='any')\n",
    "        df.to_csv(path_to_dataset_folder +'/processed_'+file,index= False)\n",
    "        print('File: ' + file)\n",
    "        print(len(df))\n",
    "        print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Printing the head() and other info of the processed CSV files for visual inspection.\n",
    "files = os.listdir(path_to_ready_csv)\n",
    "for tempfile in files:\n",
    "    df = pd.read_table(path_to_ready_csv + '/' + tempfile, header = 0, sep = ',')\n",
    "    print('File: ' + tempfile + ' | Lines:' + str(len(df)))\n",
    "    types = df.dtypes\n",
    "    cardi = df.apply(pd.Series.nunique)\n",
    "    df2=pd.DataFrame({'Types': types,\n",
    "                      'Cardinality': cardi})\n",
    "    print(df2)\n",
    "    print('\\n')\n",
    "    print(df.head())\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

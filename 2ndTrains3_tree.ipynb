{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sklearn.tree as sktr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "#Paths to access the necessary data\n",
    "path_to_dataset_folder = 'C:/Users/Dimiexe/Desktop/DATAsets'\n",
    "tree_meta_feature_path = path_to_dataset_folder + '/DATA/tree_metafeatures_for_test_CSVs'\n",
    "ground_truth_path = path_to_dataset_folder + '/DATA/ground_truth'\n",
    "metaModel_data_path = path_to_dataset_folder + '/DATA/metaModel_Datasets'\n",
    "manual_meta_feature_path = path_to_dataset_folder + '/DATA/manual_metafeatures_for_test_CSVs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines the extracted metafeatures and ground truth for each dataset into a new dataset.\n",
    "#Every line contains the 16 metafeatures + 2 (3 for xgboost) ground truth hyperparameters and represents\n",
    "#a different dataset. Returns that unified dataset.\n",
    "def build_dataSet_DL(tree_meta_feature_path, ground_truth_path, manual_meta_feature_path = None,\n",
    "                     algo = 'RF', method = 'drop', eco = True):\n",
    "    for i,f in enumerate(os.listdir(tree_meta_feature_path + '/' + method)):\n",
    "        line = pd.read_csv(tree_meta_feature_path + '/' + method + '/' + f,\n",
    "                           names = ['TreeDiam', 'TreeHeight', 'TotalNodes', 'TotalLeaves',\n",
    "                                    'maxNodePerLevel', 'meanNodePerLevel', 'stdNodePerLevel',\n",
    "                                    'ShortBranch', 'meanBranch', 'stdBranch','maxFeatureFreq',\n",
    "                                    'minFeatureFreq', 'meanFeatureFreq', 'stdFeatureFreq','NaNsPerLine'\n",
    "                                   ],\n",
    "                           header = None, sep=',')\n",
    "        if not manual_meta_feature_path == None:\n",
    "            line2 = pd.read_csv(manual_meta_feature_path + '/' + method + '/' + f,\n",
    "                                names = ['nr_inst', 'nr_attr', 'inst_to_attr', 'nr_cat', 'nr_num', 'cat_to_num',\n",
    "                                         'nr_class_mean', 'nr_class_std', 'cor_mean', 'cor_std', 'cov_mean', 'cov_std',\n",
    "                                         'kurtosis_mean', 'kurtosis_std', 'mad_mean', 'mad_std', 'max_mean', 'max_std',\n",
    "                                         'mean_mean', 'mean_std', 'median_mean', 'median_std', 'min_mean', 'min_std',\n",
    "                                         'std_mean', 'std_std', 'skew_mean', 'skew_std','var_mean', 'var_std'\n",
    "                                        ],\n",
    "                                header = None, sep=',')\n",
    "            line = pd.concat([line, line2], ignore_index = False, axis = 1)\n",
    "        line2 = pd.read_csv(ground_truth_path + '/' + algo + '/' + f, header = 0, sep=',')\n",
    "        if algo == 'GLM':\n",
    "            line2.drop(line2.columns[2:], axis = 1, inplace = True)                \n",
    "        elif algo == 'RF':\n",
    "            if eco:\n",
    "                line2.drop(line2.columns[:3], axis = 1, inplace = True)\n",
    "            line2.drop(line2.columns[2:], axis = 1, inplace = True)                \n",
    "        elif algo == 'XGboost':\n",
    "            if eco:\n",
    "                line2.drop(line2.columns[:4], axis = 1, inplace = True)\n",
    "            line2.drop(line2.columns[3:], axis = 1, inplace = True)\n",
    "        else:\n",
    "            print(\"\\n>>> Algorithm '\" + algo + \"' not valid. <<<\")\n",
    "            return None\n",
    "        line = pd.concat([line, line2], ignore_index = False, axis = 1)\n",
    "        if i==0:\n",
    "            df = line\n",
    "        else:\n",
    "            df = pd.concat([df, line], ignore_index = True, axis = 0)\n",
    "    df.to_csv(metaModel_data_path + '/' + algo + '_' + method + '.csv', sep =',', index = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_prep_tree(data, algo):\n",
    "    if algo in ['RF', 'GLM']:\n",
    "        y = data.iloc[:, -2:]\n",
    "        x = data.iloc[:, 0:-2]\n",
    "    elif algo == 'XGboost':\n",
    "        y = data.iloc[:, -3:]\n",
    "        x = data.iloc[:, 0:-3]\n",
    "    else:\n",
    "        print('\\n>>> Not supported algorithm. Returning None! <<<')\n",
    "        return None\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns \"train\" and \"test\" pandas.DataFrames split acording to the provided ratio.\n",
    "def split_train_test_indices(dataset, split_ratio = .15):\n",
    "    import random\n",
    "    rows = dataset.shape[0]\n",
    "    test_rows = round(rows * split_ratio)\n",
    "    train_rows = rows - test_rows\n",
    "    \n",
    "    #Generate train_data\n",
    "    train_indices = []\n",
    "    while len(train_indices) < train_rows:\n",
    "        candidate = random.randint(0, rows-1)\n",
    "        if candidate not in train_indices:\n",
    "            train_indices.append(candidate)\n",
    "    \n",
    "    #Generate test data\n",
    "    test_indices = []\n",
    "    for j in range(rows):\n",
    "        if j not in train_indices:\n",
    "            test_indices.append(j)    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "def split_train_test(dataset, split_ratio = .15):\n",
    "    import random\n",
    "    rows = len(dataset)\n",
    "    test_rows = round(rows * split_ratio)\n",
    "    train_rows = rows - test_rows\n",
    "    \n",
    "    #Generate train_data\n",
    "    train_indices = []\n",
    "    while len(train_indices) < train_rows:\n",
    "        candidate = random.randint(0, rows-1)\n",
    "        if candidate not in train_indices:\n",
    "            train_indices.append(candidate)\n",
    "    train = dataset.loc[train_indices,:]\n",
    "    train.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    #Generate test data\n",
    "    test_indices = []\n",
    "    for j in range(rows):\n",
    "        if j not in train_indices:\n",
    "            test_indices.append(j)\n",
    "    test = dataset.loc[test_indices,:]\n",
    "    test.reset_index(drop=True, inplace = True)    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_preds_calcs(param_vector, single_pred):\n",
    "    param_vector2 = [abs(x - single_pred) for x in param_vector]\n",
    "    temp = 0   #Store parameter value\n",
    "    ind = 100000  #Store parameter comparison value\n",
    "    for i,pa in enumerate(param_vector2):\n",
    "        if pa < ind:\n",
    "            ind = pa\n",
    "            temp = param_vector[i]\n",
    "    return temp\n",
    "\n",
    "def fix_preds(preds, algo, index, lambdas):\n",
    "    in_preds = []\n",
    "    length = len(preds)\n",
    "    if algo == 'GLM':\n",
    "        if index == 0:\n",
    "            y = range(0, 1100, 125)\n",
    "            param_alpha = [x / 1000 for x in y]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_alpha, preds[i]))\n",
    "        elif index == 1:\n",
    "            param_lambda = set(lambdas)\n",
    "            param_lambda = list(param_lambda)  #Create list of unique train lambda values\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_lambda, preds[i]))\n",
    "                \n",
    "    elif algo =='RF':\n",
    "        if index == 0:\n",
    "            param_tree = [25,50,75,100,200,300,400,500]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_tree, preds[i]))\n",
    "        elif index == 1:\n",
    "            param_depth = [20,40,60,80]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_depth, preds[i]))\n",
    "    elif algo == 'XGboost':\n",
    "        if index == 0:\n",
    "            param_tree = [25,50,100,200]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_tree, preds[i]))\n",
    "        elif index == 1:\n",
    "            param_depth = [6,10,15]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_depth, preds[i]))\n",
    "        elif index == 2:\n",
    "            colsample_bytree = [.6,.7,.8,.9]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(colsample_bytree, preds[i]))\n",
    "    return in_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_val(x_train, y_train, estimator):\n",
    "    y_cols = len(y_train.columns)\n",
    "    for target_column in range(y_cols):\n",
    "        temp =[]\n",
    "        for i in range(20):\n",
    "            results = cross_val_score(estimator,\n",
    "                                      X = x_train,\n",
    "                                      y = y_train.iloc[:,target_column],\n",
    "                                      cv = kfold,\n",
    "                                      scoring = 'neg_root_mean_squared_error'\n",
    "                                     )\n",
    "            temp = np.concatenate((temp,results))\n",
    "        print(\"%.8f | %.8f | %.8f\" % (abs(results.mean()), results.std(), abs(results.mean()) + results.std()))\n",
    "\n",
    "def do_fit_predict(x_train, y_train, x_test, y_test, estimator, plot_true_preds = False):\n",
    "    y_cols = len(y_train.columns)\n",
    "    fig,a =  plt.subplots(1,y_cols,figsize=(20, 5))\n",
    "    x = range(len(y_test))\n",
    "    for target_column in range(y_cols):\n",
    "        estimator.fit(x_train,y_train.iloc[:,target_column])\n",
    "        y_pred = estimator.predict(x_test)\n",
    "        #--------------------- Used for rounding to quantized vlues ---------------------#\n",
    "        if algo == 'GLM' and target_column == 1:                                         #\n",
    "            y_lambda_values = pd.concat([y_train.iloc[:,1],y_test.iloc[:,1]], axis=0)    #\n",
    "            preds_fixed = fix_preds(y_pred, algo, target_column, y_lambda_values.values) #\n",
    "        else:                                                                            #\n",
    "            preds_fixed = fix_preds(y_pred, algo, target_column, lambdas = None)         #\n",
    "        #--------------------------------------------------------------------------------#\n",
    "        #Plotting ground truth\n",
    "        a[target_column].plot(x,y_test.iloc[:,target_column],'bo')\n",
    "        #Plotting quantized predictions\n",
    "        a[target_column].plot(x,preds_fixed,'yx')\n",
    "        #Plotting true predictions\n",
    "        if plot_true_preds:\n",
    "            a[target_column].plot(x,y_pred,'r+')\n",
    "            a[target_column].legend(['ground truth', 'quantized preds', 'real preds'], loc='upper left')\n",
    "        else:\n",
    "            a[target_column].legend(['ground truth', 'quantized preds'], loc='upper left')                        \n",
    "        print('RMSE for hyperparam \"' + y_test.columns[target_column] + '\": '+ str(metrics.mean_squared_error(y_test.iloc[:,target_column], preds_fixed, squared = False)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this once for each cross validation AND prediction session, as the row\n",
    "#indices are stored in \"train_indices\" and \"test_indices\" and are the same\n",
    "#for both crossvalidation and fit processes.\n",
    "\n",
    "manual_indices = True\n",
    "\n",
    "split_ratio = .15 #Training/Testing set ratio.\n",
    "kfold = KFold(n_splits=10)\n",
    "#Creating the dataset once to get its dimensions. We need the number of rows, not columns,\n",
    "#so any \"algo\" is aplicable. We dont need the values of the rows, just the rows, so any\n",
    "#\"method\" is also aplicable as is the option for eco_best, or absolute_best.\n",
    "#Also we dont care about the number of metafeatures contained in the dataset, as they\n",
    "#make up the columns not the rows.\n",
    "dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                           ground_truth_path,\n",
    "                           algo = 'RF', method = 'drop', eco = True)\n",
    "#Get the indices for test and train instances based on extracted dataset dimensions.\n",
    "if manual_indices:\n",
    "    train_indices = [28,58,31,29,20,147,157,155,62,159,137,112,118,134,164,90,142,19,60,161,94,18,84,67,56,61,74,30,52,2,117,1,126,65,70,163,73,114,27,139,63,89,154,166,46,6,37,116,8,173,140,13,122,39,135,174,14,36,24,104,124,130,16,141,7,149,167,79,43,69,53,9,77,12,98,22,165,93,85,11,100,108,3,4,145,123,144,10,119,105,129,92,80,33,71,99,107,25,148,97,106,42,109,82,158,51,152,50,168,40,0,146,86,153,41,156,170,136,35,57,143,44,83,102,54,26,177,131,169,120,138,81,68,133,175,132,32,45,78,76,48,59,15,88,113,64,96,171,128,66,75]\n",
    "    test_indices = [5,17,21,23,34,38,47,49,55,72,87,91,95,101,103,110,111,115,121,125,127,150,151,160,162,172,176]\n",
    "else:\n",
    "    train_indices, test_indices = split_train_test_indices(dataset, split_ratio = .15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fit final models and make predictions.\n",
    "meta = 'all' #All final models use 45 metafeatures.\n",
    "eco = True  #All final models use eco forest option.\n",
    "for algo in ['GLM', 'RF', 'XGboost']:\n",
    "    print('------ Algo = ' + algo + ' ------')\n",
    "    if algo == 'XGboost':\n",
    "        method = 'drop'\n",
    "    else:\n",
    "        method = 'drop'\n",
    "    print('NaN method: ' + method)\n",
    "    print('Meta-features used: 45')\n",
    "    if meta == 'all':\n",
    "        dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                   ground_truth_path,\n",
    "                                   manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                   algo = algo, method = method, eco = eco)            \n",
    "    elif meta == 'tree':\n",
    "        dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                   ground_truth_path,\n",
    "                                   algo = algo, method = method, eco = eco)\n",
    "    #Creating train and test datasets according to predefined row indices.\n",
    "    train = dataset.loc[train_indices,:]\n",
    "    train.reset_index(drop=True, inplace = True)\n",
    "    test = dataset.loc[test_indices,:]\n",
    "    test.reset_index(drop=True, inplace = True)\n",
    "        \n",
    "    x_train, y_train = Data_prep_tree(train, algo)\n",
    "    x_test, y_test = Data_prep_tree(test, algo)\n",
    "    regr = RandomForestRegressor(random_state = 8328,\n",
    "                                 min_impurity_decrease = 1e-06,\n",
    "                                 max_depth = 10,\n",
    "                                 n_estimators = 1000,\n",
    "                                 max_features = .8\n",
    "                                )\n",
    "    do_fit_predict(x_train = x_train, y_train = y_train,\n",
    "                   x_test = x_test, y_test = y_test,\n",
    "                   estimator = regr, plot_true_preds = True\n",
    "                  )\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Try different combinations in cv mode to find final models.\n",
    "#    >>>  DONE  <<<\n",
    "for meta in ['tree', 'all']:\n",
    "    for eco in [True, False]:\n",
    "        if meta == 'tree':\n",
    "            print('Metafeature set: tree-metafeatures (15)')\n",
    "        elif meta == 'all':\n",
    "            print('Metafeature set: tree & manual metafeatures (45)')\n",
    "        print('Use eco ground truth:', eco)\n",
    "        print(' ')\n",
    "        for algo in ['GLM', 'RF', 'XGboost']:\n",
    "            print('------ Algo = ' + algo + ' ------')    \n",
    "            for method in ['drop', 'mean', 'median']:\n",
    "                print('NaN method: ' + method)\n",
    "                if meta == 'all':\n",
    "                    dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                               ground_truth_path,\n",
    "                                               manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                               algo = algo, method = method, eco = eco)            \n",
    "                elif meta == 'tree':\n",
    "                    dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                               ground_truth_path,\n",
    "                                               algo = algo, method = method, eco = eco)\n",
    "                #Creating train and test datasets according to predefined row indices.\n",
    "                train = dataset.loc[train_indices,:]\n",
    "                train.reset_index(drop=True, inplace = True)\n",
    "                test = dataset.loc[test_indices,:]\n",
    "                test.reset_index(drop=True, inplace = True)\n",
    "        \n",
    "                x_train, y_train = Data_prep_tree(train, algo)\n",
    "                x_test, y_test = Data_prep_tree(test, algo)\n",
    "                regr = RandomForestRegressor(random_state = 8328,\n",
    "                                             min_impurity_decrease = 1e-06,\n",
    "                                             max_depth = 10,\n",
    "                                             n_estimators = 1000,\n",
    "                                             max_features = .8\n",
    "                                            )\n",
    "                do_cross_val(x_train = x_train, y_train = y_train, estimator = regr)\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

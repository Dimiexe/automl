{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.optimizers as opts\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn import tree as sktr\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "#Paths to access the necessary data\n",
    "path_to_dataset_folder = 'C:/Users/Dimiexe/Desktop/DATAsets'\n",
    "tree_meta_feature_path = path_to_dataset_folder + '/DATA/tree_metafeatures_for_test_CSVs'\n",
    "ground_truth_path = path_to_dataset_folder + '/DATA/ground_truth'\n",
    "metaModel_data_path = path_to_dataset_folder + '/DATA/metaModel_Datasets'\n",
    "regularization_path = path_to_dataset_folder + '/regularization'\n",
    "normalization_path = path_to_dataset_folder + '/normalization'\n",
    "model_path = path_to_dataset_folder + '/final_models'\n",
    "manual_meta_feature_path = path_to_dataset_folder + '/DATA/manual_metafeatures_for_test_CSVs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines the extracted metafeatures and ground truth for each dataset into a new dataset.\n",
    "#Every line contains the 16 metafeatures + 2 (3 for xgboost) ground truth hyperparameters and represents\n",
    "#a different dataset. Returns that unified dataset.\n",
    "def build_dataSet_DL(tree_meta_feature_path, ground_truth_path, manual_meta_feature_path = None,\n",
    "                     algo = 'RF', method = 'drop', eco = True):\n",
    "    for i,f in enumerate(os.listdir(tree_meta_feature_path + '/' + method)):\n",
    "        line = pd.read_csv(tree_meta_feature_path + '/' + method + '/' + f,\n",
    "                           names = ['TreeDiam', 'TreeHeight', 'TotalNodes', 'TotalLeaves',\n",
    "                                    'maxNodePerLevel', 'meanNodePerLevel', 'stdNodePerLevel',\n",
    "                                    'ShortBranch', 'meanBranch', 'stdBranch','maxFeatureFreq',\n",
    "                                    'minFeatureFreq', 'meanFeatureFreq', 'stdFeatureFreq','NaNsPerLine'\n",
    "                                   ],\n",
    "                           header = None, sep=',')\n",
    "        if not manual_meta_feature_path == None:\n",
    "            line2 = pd.read_csv(manual_meta_feature_path + '/' + method + '/' + f,\n",
    "                                names = ['nr_inst', 'nr_attr', 'inst_to_attr', 'nr_cat', 'nr_num', 'cat_to_num',\n",
    "                                         'nr_class_mean', 'nr_class_std', 'cor_mean', 'cor_std', 'cov_mean', 'cov_std',\n",
    "                                         'kurtosis_mean', 'kurtosis_std', 'mad_mean', 'mad_std', 'max_mean', 'max_std',\n",
    "                                         'mean_mean', 'mean_std', 'median_mean', 'median_std', 'min_mean', 'min_std',\n",
    "                                         'std_mean', 'std_std', 'skew_mean', 'skew_std','var_mean', 'var_std'\n",
    "                                        ],\n",
    "                                header = None, sep=',')\n",
    "            line = pd.concat([line, line2], ignore_index = False, axis = 1)\n",
    "        line2 = pd.read_csv(ground_truth_path + '/' + algo + '/' + f, header = 0, sep=',')\n",
    "        if algo == 'GLM':\n",
    "            line2.drop(line2.columns[2:], axis = 1, inplace = True)                \n",
    "        elif algo == 'RF':\n",
    "            if eco:\n",
    "                line2.drop(line2.columns[:3], axis = 1, inplace = True)\n",
    "            line2.drop(line2.columns[2:], axis = 1, inplace = True)                \n",
    "        elif algo == 'XGboost':\n",
    "            if eco:\n",
    "                line2.drop(line2.columns[:4], axis = 1, inplace = True)\n",
    "            line2.drop(line2.columns[3:], axis = 1, inplace = True)\n",
    "        else:\n",
    "            print(\"\\n>>> Algorithm '\" + algo + \"' not valid. <<<\")\n",
    "            return None\n",
    "        line = pd.concat([line, line2], ignore_index = False, axis = 1)\n",
    "        if i==0:\n",
    "            df = line\n",
    "        else:\n",
    "            df = pd.concat([df, line], ignore_index = True, axis = 0)\n",
    "    df.to_csv(metaModel_data_path + '/' + algo + '_' + method + '.csv', sep =',', index = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_dataset(data, algo, method, regularize_y, regularization_path):\n",
    "    regularization_means = pd.read_csv(regularization_path + '/regularization_means_' + algo + '_' + method + '.csv')\n",
    "    regularization_stds = pd.read_csv(regularization_path + '/regularization_stds_' + algo + '_' + method + '.csv')\n",
    "    if regularize_y:\n",
    "        L = data.columns\n",
    "    else:\n",
    "        if algo in ['RF', 'GLM']:\n",
    "            L = data.columns[0:-2]\n",
    "        elif algo == 'XGboost':\n",
    "            L = data.columns[0:-3]\n",
    "    for j in L:\n",
    "        for i,x in enumerate(data.loc[:,j]):\n",
    "            data.loc[i,j] = (x-regularization_means.loc[0,j])/regularization_stds.loc[0,j]\n",
    "    return data\n",
    "\n",
    "def normalize_dataset(data, algo, method, normalize_y, normalization_path):\n",
    "    normal_mins = pd.read_csv(normalization_path + '/normalizaton_mins_' + algo + '_' + method + '.csv')\n",
    "    normal_maxes = pd.read_csv(normalization_path + '/normalizaton_maxes_' + algo + '_' + method + '.csv')\n",
    "    if normalize_y:\n",
    "        L = data.columns\n",
    "    else:\n",
    "        if algo in ['RF', 'GLM']:\n",
    "            L = data.columns[0:-2]\n",
    "        elif algo == 'XGboost':\n",
    "            L = data.columns[0:-3]\n",
    "    for j in L:\n",
    "        for i,x in enumerate(data.loc[:,j]):\n",
    "            data.loc[i,j] = ( x-normal_mins.loc[0,j] ) / (normal_maxes.loc[0,j] - normal_mins.loc[0,j])\n",
    "    return data\n",
    "\n",
    "def deregularize_preds(preds, algo, method, regularization_path):\n",
    "    regularization_means = pd.read_csv(regularization_path + '/regularization_means_' + algo + '_' + method + '.csv')\n",
    "    regularization_stds = pd.read_csv(regularization_path + '/regularization_stds_' + algo + '_' + method + '.csv')\n",
    "    if algo in ['RF','GLM']:\n",
    "        L = [-1,-2]\n",
    "    elif algo == 'XGboost':\n",
    "        L = [-1,-2,-3]\n",
    "    for j in L:\n",
    "        for i,x in enumerate(preds[:,j]):\n",
    "            preds[i,j] = (x * regularization_stds.iloc[0,j] + regularization_means.iloc[0,j])\n",
    "    return preds\n",
    "\n",
    "def denormalize_preds(preds, algo, method, normalization_path):    \n",
    "    normal_mins = pd.read_csv(normalization_path + '/normalizaton_mins_' + algo + '_' + method + '.csv')\n",
    "    normal_maxes = pd.read_csv(normalization_path + '/normalizaton_maxes_' + algo + '_' + method + '.csv')\n",
    "    if algo in ['RF','GLM']:\n",
    "        L = [-1,-2]\n",
    "    elif algo == 'XGboost':\n",
    "        L = [-1,-2,-3]\n",
    "    for j in L:\n",
    "        for i,x in enumerate(preds[:,j]):\n",
    "            preds[i,j] = x * (normal_maxes.iloc[0,j] - normal_mins.iloc[0,j]) + normal_mins.iloc[0,j]\n",
    "    return preds\n",
    "\n",
    "# Standardize data and splits to predictors (x) and targets (y)\n",
    "# Standardization options:\n",
    "# 'regularize' for regularization (mean = 0, std = 1)\n",
    "# 'normalize' for normalization (range of values in [0,1])\n",
    "def Data_prep_NN(data, algo, method, regularization_path, normalization_path, standardize = 'regularize', regularize_y = False, normalize_y = False):\n",
    "    if standardize == 'regularize':\n",
    "        data = regularize_dataset(data, algo, method, regularize_y, regularization_path)\n",
    "    elif standardize == 'normalize':\n",
    "        data = normalize_dataset(data, algo, method, normalize_y, normalization_path)\n",
    "    else:\n",
    "        print('No regularization performed. Results will be sub-optimal.')\n",
    "    data = data.values\n",
    "    if algo in ['RF', 'GLM']:\n",
    "        y = data[:, -2:]\n",
    "        x = data[:, 0:-2]\n",
    "    elif algo == 'XGboost':\n",
    "        y = data[:, -3:]\n",
    "        x = data[:, 0:-3]\n",
    "    else:\n",
    "        print('\\n>>> Not supported algorithm. Returning None! <<<')\n",
    "        return None\n",
    "    return x, y\n",
    "\n",
    "def Data_prep_RF(data, algo):\n",
    "    if algo in ['RF', 'GLM']:\n",
    "        y = data.iloc[:, -2:]\n",
    "        x = data.iloc[:, 0:-2]\n",
    "    elif algo == 'XGboost':\n",
    "        y = data.iloc[:, -3:]\n",
    "        x = data.iloc[:, 0:-3]\n",
    "    else:\n",
    "        print('\\n>>> Not supported algorithm. Returning None! <<<')\n",
    "        return None\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns \"train\" and \"test\" pandas.DataFrames split acording to the provided ratio.\n",
    "def split_train_test_indices(dataset, split_ratio = .15):\n",
    "    import random\n",
    "    rows = dataset.shape[0]\n",
    "    test_rows = round(rows * split_ratio)\n",
    "    train_rows = rows - test_rows\n",
    "    \n",
    "    #Generate train_data\n",
    "    train_indices = []\n",
    "    while len(train_indices) < train_rows:\n",
    "        candidate = random.randint(0, rows-1)\n",
    "        if candidate not in train_indices:\n",
    "            train_indices.append(candidate)\n",
    "    \n",
    "    #Generate test data\n",
    "    test_indices = []\n",
    "    for j in range(rows):\n",
    "        if j not in train_indices:\n",
    "            test_indices.append(j)    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "def split_train_test(dataset, split_ratio = .15):\n",
    "    import random\n",
    "    rows = len(dataset)\n",
    "    test_rows = round(rows * split_ratio)\n",
    "    train_rows = rows - test_rows\n",
    "    \n",
    "    #Generate train_data\n",
    "    train_indices = []\n",
    "    while len(train_indices) < train_rows:\n",
    "        candidate = random.randint(0, rows-1)\n",
    "        if candidate not in train_indices:\n",
    "            train_indices.append(candidate)\n",
    "    train = dataset.loc[train_indices,:]\n",
    "    train.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    #Generate test data\n",
    "    test_indices = []\n",
    "    for j in range(rows):\n",
    "        if j not in train_indices:\n",
    "            test_indices.append(j)\n",
    "    test = dataset.loc[test_indices,:]\n",
    "    test.reset_index(drop=True, inplace = True)    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_preds_calcs(param_vector, single_pred):\n",
    "    param_vector2 = [abs(x - single_pred) for x in param_vector]\n",
    "    temp = 0   #Store parameter value\n",
    "    ind = 100000  #Store parameter comparison value\n",
    "    for i,pa in enumerate(param_vector2):\n",
    "        if pa < ind:\n",
    "            ind = pa\n",
    "            temp = param_vector[i]\n",
    "    return temp\n",
    "\n",
    "def fix_preds_RF(preds, algo, index, lambdas):\n",
    "    in_preds = []\n",
    "    length = len(preds)\n",
    "    if algo == 'GLM':\n",
    "        if index == 0:\n",
    "            y = range(0, 1100, 125)\n",
    "            param_alpha = [x / 1000 for x in y]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_alpha, preds[i]))\n",
    "        elif index == 1:\n",
    "            param_lambda = set(lambdas)\n",
    "            param_lambda = list(param_lambda)  #Create list of unique train lambda values\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_lambda, preds[i]))\n",
    "                \n",
    "    elif algo =='RF':\n",
    "        if index == 0:\n",
    "            param_tree = [25,50,75,100,200,300,400,500]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_tree, preds[i]))\n",
    "        elif index == 1:\n",
    "            param_depth = [20,40,60,80]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_depth, preds[i]))\n",
    "    elif algo == 'XGboost':\n",
    "        if index == 0:\n",
    "            param_tree = [25,50,100,200]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_tree, preds[i]))\n",
    "        elif index == 1:\n",
    "            param_depth = [6,10,15]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_depth, preds[i]))\n",
    "        elif index == 2:\n",
    "            colsample_bytree = [.6,.7,.8,.9]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(colsample_bytree, preds[i]))\n",
    "    return in_preds\n",
    "\n",
    "def fix_preds_NN(preds, algo, lambdas):\n",
    "    in_preds = pd.DataFrame({})\n",
    "    length = len(preds)\n",
    "    temp0 = []\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    if algo == 'GLM':\n",
    "        y = range(0, 1100, 125)\n",
    "        param_alpha = [x / 1000 for x in y]\n",
    "        param_lambda = set(lambdas)\n",
    "        param_lambda = list(param_lambda)  #Create list of unique train lambda values\n",
    "        for i in range(length):\n",
    "            temp0.append(fix_preds_calcs(param_alpha, preds[i,0]))\n",
    "            temp1.append(fix_preds_calcs(param_lambda, preds[i,1]))\n",
    "        in_preds['A'] = temp0\n",
    "        in_preds['B'] = temp1\n",
    "                \n",
    "    elif algo =='RF':\n",
    "        param_tree = [25,50,75,100,200,300,400,500]\n",
    "        param_depth = [20,40,60,80]\n",
    "        for i in range(length):\n",
    "            temp0.append(fix_preds_calcs(param_tree, preds[i,0]))\n",
    "            temp1.append(fix_preds_calcs(param_depth, preds[i,1]))\n",
    "        in_preds['A'] = temp0\n",
    "        in_preds['B'] = temp1\n",
    "        \n",
    "    elif algo == 'XGboost':\n",
    "        for i in range(length):\n",
    "            param_tree = [25,50,100,200]\n",
    "            param_depth = [6,10,15]\n",
    "            colsample_bytree = [.6,.7,.8,.9]\n",
    "            temp0.append(fix_preds_calcs(param_tree, preds[i,0]))\n",
    "            temp1.append(fix_preds_calcs(param_depth, preds[i,1]))\n",
    "            temp2.append(fix_preds_calcs(colsample_bytree, preds[i,2]))\n",
    "        in_preds['A'] = temp0\n",
    "        in_preds['B'] = temp1\n",
    "        in_preds['C'] = temp2\n",
    "    return in_preds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fit_predict_RF(algo, x_train, y_train, x_test, y_test, estimator, plot_true_preds = True):\n",
    "    y_cols = len(y_train.columns)\n",
    "    fig,a =  plt.subplots(1,y_cols,figsize=(20, 5))\n",
    "    x = range(len(y_test))\n",
    "    predictions = []\n",
    "    for i,target_column in enumerate(range(y_cols)):\n",
    "        estimator.fit(x_train,y_train.iloc[:,target_column])\n",
    "        y_pred = estimator.predict(x_test)\n",
    "        predictions.append(y_pred)\n",
    "        #print(\"RMSE:\",metrics.mean_squared_error(y_test.iloc[:,target_column], y_pred, squared = False))\n",
    "        #Plotting ground truth\n",
    "        a[target_column].plot(x,y_test.iloc[:,target_column],'bo')\n",
    "        #---------------------- Used for rounding to quantized vlues -----------------------#\n",
    "        if algo == 'GLM' and target_column == 1:                                            #\n",
    "            y_lambda_values = pd.concat([y_train.iloc[:,1],y_test.iloc[:,1]], axis=0)       #\n",
    "            preds_fixed = fix_preds_RF(y_pred, algo, target_column, y_lambda_values.values) #\n",
    "        else:                                                                               #\n",
    "            preds_fixed = fix_preds_RF(y_pred, algo, target_column, lambdas = None)         #\n",
    "        #-----------------------------------------------------------------------------------#\n",
    "        print('RMSE for hyperparam \"' + y_test.columns[target_column] + '\": '+ str(metrics.mean_squared_error(y_test.iloc[:,target_column], preds_fixed, squared = False)))\n",
    "        #Plotting quantized predictions\n",
    "        a[target_column].plot(x,preds_fixed,'yx')\n",
    "        #Plotting true predictions\n",
    "        if plot_true_preds:\n",
    "            a[target_column].plot(x,y_pred,'r+')\n",
    "            a[target_column].legend(['ground truth', 'quantized preds', 'real preds'], loc='upper left')\n",
    "        else:\n",
    "            a[target_column].legend(['ground truth', 'quantized preds'], loc='upper left')    \n",
    "    plt.show()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict_RF(algo, x_train, y_train, x_test, y_test, estimator, plot_true_preds = True):\n",
    "    y_cols = len(y_train.columns)\n",
    "    fig,a =  plt.subplots(1,y_cols,figsize=(20, 5))\n",
    "    x = range(len(y_test))\n",
    "    predictions = []\n",
    "    for i,target_column in enumerate(range(y_cols)):\n",
    "        y_pred = estimator.predict(x_test)\n",
    "        predictions.append(y_pred)\n",
    "        #print(\"RMSE:\",metrics.mean_squared_error(y_test.iloc[:,target_column], y_pred, squared = False))\n",
    "        #Plotting ground truth\n",
    "        a[target_column].plot(x,y_test.iloc[:,target_column],'bo')\n",
    "        #---------------------- Used for rounding to quantized vlues -----------------------#\n",
    "        if algo == 'GLM' and target_column == 1:                                            #\n",
    "            y_lambda_values = pd.concat([y_train.iloc[:,1],y_test.iloc[:,1]], axis=0)       #\n",
    "            preds_fixed = fix_preds_RF(y_pred, algo, target_column, y_lambda_values.values) #\n",
    "        else:                                                                               #\n",
    "            preds_fixed = fix_preds_RF(y_pred, algo, target_column, lambdas = None)         #\n",
    "        #-----------------------------------------------------------------------------------#\n",
    "        print('RMSE for hyperparam \"' + y_test.columns[target_column] + '\": '+ str(metrics.mean_squared_error(y_test.iloc[:,target_column], preds_fixed, squared = False)))\n",
    "        #Plotting quantized predictions\n",
    "        a[target_column].plot(x,preds_fixed,'yx')\n",
    "        #Plotting true predictions\n",
    "        if plot_true_preds:\n",
    "            a[target_column].plot(x,y_pred,'r+')\n",
    "            a[target_column].legend(['ground truth', 'quantized preds', 'real preds'], loc='upper left')\n",
    "        else:\n",
    "            a[target_column].legend(['ground truth', 'quantized preds'], loc='upper left')    \n",
    "    plt.show()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains and returns the deep learning estimator for the specified algorithm.\n",
    "#The below code is based on the code found at:\n",
    "#https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "#and is tweked acording to the needs of this project.\n",
    "\n",
    "# define function to make base model\n",
    "def create_model(input_dim = 45, algo = 'GLM', hidden = 5, two_layers = False, half_on_second = False):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    if two_layers:\n",
    "        if half_on_second:\n",
    "            model.add(Dense(round(hidden/2), kernel_initializer='normal'))\n",
    "        else:\n",
    "            model.add(Dense(hidden, kernel_initializer='normal'))\n",
    "    if algo in ['GLM', 'RF']:\n",
    "        model.add(Dense(2, kernel_initializer='normal'))\n",
    "    elif algo == 'XGboost':\n",
    "        model.add(Dense(3, kernel_initializer='normal'))\n",
    "    else:\n",
    "        print(\"\\n>>> Algorithm '\" + algo + \"' not valid. <<<\")\n",
    "        return None\n",
    "    # Compile model\n",
    "    model.compile(loss = 'mse', optimizer = opts.Adam(learning_rate=0.01))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this once for each cross validation AND prediction session, as the row\n",
    "#indices are stored in \"train_indices\" and \"test_indices\" and are the same\n",
    "#for both crossvalidation and fit processes.\n",
    "\n",
    "manual_indices = True\n",
    "\n",
    "split_ratio = .15 #Training/Testing set ratio.\n",
    "kfold = KFold(n_splits=10)\n",
    "#Creating the dataset once to get its dimensions. We need the number of rows, not columns,\n",
    "#so any \"algo\" is aplicable. We dont need the values of the rows, just the rows, so any\n",
    "#\"method\" is also aplicable as is the option for eco_best, or absolute_best.\n",
    "#Also we dont care about the number of metafeatures contained in the dataset, as they\n",
    "#make up the columns not the rows.\n",
    "#Get the indices for test and train instances based on extracted dataset dimensions.\n",
    "if manual_indices:\n",
    "    train_indices = [28,58,31,29,20,147,157,155,62,159,137,112,118,134,164,90,142,19,60,161,94,18,84,67,56,61,74,30,52,2,117,1,126,65,70,163,73,114,27,139,63,89,154,166,46,6,37,116,8,173,140,13,122,39,135,174,14,36,24,104,124,130,16,141,7,149,167,79,43,69,53,9,77,12,98,22,165,93,85,11,100,108,3,4,145,123,144,10,119,105,129,92,80,33,71,99,107,25,148,97,106,42,109,82,158,51,152,50,168,40,0,146,86,153,41,156,170,136,35,57,143,44,83,102,54,26,177,131,169,120,138,81,68,133,175,132,32,45,78,76,48,59,15,88,113,64,96,171,128,66,75]\n",
    "    test_indices = [5,17,21,23,34,38,47,49,55,72,87,91,95,101,103,110,111,115,121,125,127,150,151,160,162,172,176]\n",
    "else:\n",
    "    dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                               ground_truth_path,\n",
    "                               algo = 'RF', method = 'drop', eco = True)\n",
    "    train_indices, test_indices = split_train_test_indices(dataset, split_ratio = .15)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fit final meta-models and make predictions.\n",
    "RF_combine_weight = .5\n",
    "NN_combine_weight = 1 - RF_combine_weight\n",
    "plot_true_preds = True\n",
    "print_model_params = True\n",
    "load_models = True\n",
    "RF_models = {}\n",
    "RF_preds = {}\n",
    "NN_preds = {}\n",
    "final_preds = {}\n",
    "times = {}\n",
    "\n",
    "for algo in ['GLM', 'RF', 'XGboost']:\n",
    "##################### Load models #####################\n",
    "    if load_models:\n",
    "        for i in os.listdir(model_path):\n",
    "            model_name = i.split('/')[-1].split('.')[0]\n",
    "            prefix = model_name.split('_')[0]\n",
    "            algorithm = model_name.split('_')[2]\n",
    "            if algorithm == algo:\n",
    "                if prefix == 'RF':\n",
    "                    param = model_name.split('_')[-1]\n",
    "                    key = algorithm + '_' + param\n",
    "                    RF_models[key] = joblib.load(model_path + '/'+ i)\n",
    "                else:\n",
    "                    NN_model = load_model(model_path + '/'+ i)    \n",
    "    \n",
    "##################### RF metamodel section #####################\n",
    "    print('------ Algo = ' + algo + ' ------')\n",
    "    eco = True  #All final RF models use eco forest option.\n",
    "    if algo == 'GLM':\n",
    "        meta = ['all', 'tree']\n",
    "        method = ['drop', 'median']\n",
    "    elif algo == 'RF':\n",
    "        meta = ['tree', 'all']\n",
    "        method = ['drop', 'drop']\n",
    "    elif algo == 'XGboost':\n",
    "        meta = ['all', 'all', 'all']\n",
    "        method = ['mean', 'drop', 'drop']\n",
    "    \n",
    "    mean_time = 0\n",
    "    intermediate_times = []\n",
    "    intermediate_preds = []\n",
    "    for i in range(len(method)):\n",
    "        print('Examined parameter:', i+1)\n",
    "        if print_model_params:\n",
    "            print('NaN method: ' + method[i])\n",
    "        if meta[i] == 'all':\n",
    "            if print_model_params:\n",
    "                print('Meta-features used: 45')\n",
    "            dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                       ground_truth_path,\n",
    "                                       manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                       algo = algo, method = method[i], eco = eco\n",
    "                                      )            \n",
    "        elif meta[i] == 'tree':\n",
    "            if print_model_params:\n",
    "                print('Meta-features used: 15')\n",
    "            dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                       ground_truth_path,\n",
    "                                       algo = algo, method = method[i], eco = eco\n",
    "                                      )\n",
    "        #Creating train and test datasets according to predefined row indices.\n",
    "        train = dataset.loc[train_indices,:]\n",
    "        train.reset_index(drop=True, inplace = True)\n",
    "        test = dataset.loc[test_indices,:]\n",
    "        test.reset_index(drop=True, inplace = True)\n",
    "        \n",
    "        x_train, y_train = Data_prep_RF(train, algo)\n",
    "        x_test, y_test = Data_prep_RF(test, algo)\n",
    "                \n",
    "        print('RF meta-model results:')\n",
    "        start_time = time.time()\n",
    "        if not load_models:\n",
    "            regr = RandomForestRegressor(random_state = 8328,\n",
    "                                         min_impurity_decrease = 1e-06,\n",
    "                                         max_depth = 10,\n",
    "                                         n_estimators = 1000,\n",
    "                                         max_features = .8\n",
    "                                        )\n",
    "            batch_preds = do_fit_predict_RF(algo = algo, x_train = x_train, y_train = y_train,\n",
    "                                            x_test = x_test, y_test = y_test,\n",
    "                                            estimator = regr, plot_true_preds = plot_true_preds\n",
    "                                           )\n",
    "        else:\n",
    "            model_key = algo + '_param' + str(i+1)\n",
    "            regr = RF_models[model_key]\n",
    "            batch_preds = do_predict_RF(algo = algo, x_train = x_train, y_train = y_train,\n",
    "                                        x_test = x_test, y_test = y_test,\n",
    "                                        estimator = regr, plot_true_preds = plot_true_preds\n",
    "                                       )\n",
    "        intermediate_times.append(time.time() - start_time)\n",
    "        intermediate_preds.append(batch_preds[i])\n",
    "        #joblib.dump(regr, model_path + '/RF_model_' + algo + '_param' + str(i+1) + '.joblib')\n",
    "        print(' ')\n",
    "    mean_time = np.mean(intermediate_times)\n",
    "    RF_preds[algo] = intermediate_preds\n",
    "    \n",
    "    ##################### NN metamodel section #####################    \n",
    "    #Setting params for each algorithm's model\n",
    "    standardize = 'regularize'\n",
    "    if algo == 'GLM':\n",
    "        meta = 'tree'\n",
    "        standardize_y = False\n",
    "        two_layers = False\n",
    "        half_on_second = False\n",
    "        method = 'drop'\n",
    "        hidden = 5\n",
    "    elif algo == 'RF':\n",
    "        meta = 'tree'\n",
    "        standardize_y = False\n",
    "        two_layers = True\n",
    "        half_on_second = False\n",
    "        method = 'mean'\n",
    "        hidden = 10\n",
    "    elif algo =='XGboost':\n",
    "        meta = 'tree'\n",
    "        standardize_y = False\n",
    "        two_layers = False\n",
    "        method = 'drop'\n",
    "        hidden = 5\n",
    "        \n",
    "    #Print model info\n",
    "    if print_model_params:\n",
    "        print('Metafeature set:',meta)\n",
    "        print('Standardization:',standardize)\n",
    "        if standardize == 'regularize':\n",
    "            print('regularize_y:',standardize_y)\n",
    "        elif standardize == 'normalize':\n",
    "            print('normalize_y:',standardize_y)\n",
    "        print('Hidden neurons:', hidden)\n",
    "        print('2nd hidden layer:',two_layers)\n",
    "        if two_layers:\n",
    "            print('2nd hidden has half neurons:',half_on_second)\n",
    "        print('Algo:',algo)\n",
    "        print('Nan method:',method)\n",
    "    \n",
    "    #Prepare the data\n",
    "    if meta == 'all':\n",
    "        input_dim = 45\n",
    "        dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                   ground_truth_path,\n",
    "                                   manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                   algo = algo, method = method\n",
    "                                  )\n",
    "    elif meta == 'tree':\n",
    "        input_dim = 15\n",
    "        dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                   ground_truth_path,\n",
    "                                   algo = algo, method = method\n",
    "                                  )\n",
    "    #Creating train and test datasets according to predefined row indices. They\n",
    "    #are the same as the RF metamodel ones.\n",
    "    train = dataset.loc[train_indices,:]\n",
    "    train.reset_index(drop=True, inplace = True)\n",
    "    test = dataset.loc[test_indices,:]\n",
    "    test.reset_index(drop=True, inplace = True)\n",
    "    x_train, y_train = Data_prep_NN(train, algo, method,\n",
    "                                    regularization_path, normalization_path,\n",
    "                                    standardize = standardize,\n",
    "                                    regularize_y = standardize_y,\n",
    "                                    normalize_y = standardize_y\n",
    "                                   )\n",
    "    x_test, y_test = Data_prep_NN(test, algo, method,\n",
    "                                  regularization_path, normalization_path,\n",
    "                                  standardize = standardize\n",
    "                                 )\n",
    "    \n",
    "    #Create the model\n",
    "    model = create_model(input_dim = input_dim, algo = algo, hidden = hidden,\n",
    "                         two_layers = two_layers, half_on_second = half_on_second\n",
    "                        )\n",
    "    \n",
    "    print('NN meta-model results:')\n",
    "    \n",
    "    #Do fit and predictions session\n",
    "    es = EarlyStopping(monitor='val_loss',\n",
    "                       mode='min',\n",
    "                       patience = 400,\n",
    "                       verbose = 1\n",
    "                      )\n",
    "    mc = ModelCheckpoint(model_path + '/NN_model_' + algo + '.h5',\n",
    "                         monitor='val_loss',\n",
    "                         mode='min',\n",
    "                         verbose=0,\n",
    "                         save_best_only=True\n",
    "                        )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if not load_models:\n",
    "        history = model.fit(x = x_train, y = y_train,\n",
    "                            validation_split = 0.15,\n",
    "                            callbacks = [es, mc],\n",
    "                            epochs = 2000,\n",
    "                            batch_size = 5,\n",
    "                            verbose = 0\n",
    "                           )\n",
    "    model = load_model(model_path + '/NN_model_' + algo + '.h5')\n",
    "    NN_preds[algo] = model.predict(x_test, verbose = 0)\n",
    "    mean_time = mean_time + (time.time() - start_time)\n",
    "    times[algo] = mean_time\n",
    "    #Destandardize predictions if y was standardized.\n",
    "    if standardize == 'regularize' and standardize_y:\n",
    "        NN_preds[algo] = deregularize_preds(NN_preds[algo], algo, method, regularization_path)\n",
    "    elif standardize == 'normalize' and standardize_y:        \n",
    "        NN_preds[algo] = denormalize_preds(NN_preds[algo], algo, method, normalization_path)\n",
    "    \n",
    "    #Plot the results\n",
    "    y_cols = NN_preds[algo].shape[1]\n",
    "    fig,a =  plt.subplots(1,y_cols,figsize=(20, 5))\n",
    "    x = range(NN_preds[algo].shape[0])        \n",
    "    y_lambda_values = []\n",
    "    if algo == 'GLM':\n",
    "        y_lambda_values = np.concatenate([y_train[:,1],y_test[:,1]], axis=0)\n",
    "    preds_fixed = fix_preds_NN(NN_preds[algo], algo, y_lambda_values)\n",
    "    temp_rmse = []\n",
    "    for target_column in range(y_cols):\n",
    "        #Plot ground truth\n",
    "        a[target_column].plot(x,y_test[:,target_column],'bo')\n",
    "        a[target_column].plot(x,preds_fixed[:,target_column],'yx')\n",
    "        if plot_true_preds:\n",
    "            a[target_column].plot(x,NN_preds[algo][:,target_column],'r+')\n",
    "            a[target_column].legend(['ground truth', 'quantized preds', 'real preds'], loc='upper left')\n",
    "        else:\n",
    "            a[target_column].legend(['ground truth', 'quantized preds'], loc='upper left')                        \n",
    "    \n",
    "        #Calculate rmse separately for each parameter\n",
    "        temp2 = math.sqrt(np.mean(np.square(preds_fixed[:,target_column] - y_test[:,target_column])))\n",
    "        print('RMSE for paramenter ' + str(target_column+1) + ': '+ str(round(10000*temp2)/10000))\n",
    "    plt.show()\n",
    "    print(' ')\n",
    "                                   \n",
    "    ##################### Combining the results #####################\n",
    "    combined_preds = []\n",
    "    fig,a =  plt.subplots(1,y_cols,figsize=(20, 5))\n",
    "    print('Combined predictions:')\n",
    "    print('Elapsed time:',times[algo])\n",
    "    for i in range(y_cols):\n",
    "        true_preds = RF_preds[algo][i] * RF_combine_weight + NN_preds[algo][:,i] * NN_combine_weight\n",
    "        a[i].plot(x,y_test[:,i],'bo')\n",
    "        #-------------------- Used for rounding to quantized vlues ----------------------#\n",
    "        if algo == 'GLM' and i == 1:                                                     #\n",
    "            true_preds_fixed = fix_preds_RF(true_preds, algo, i, y_lambda_values)        #\n",
    "        else:                                                                            #\n",
    "            true_preds_fixed = fix_preds_RF(true_preds, algo, i, lambdas = None)         #\n",
    "        #--------------------------------------------------------------------------------#\n",
    "        a[i].plot(x,true_preds_fixed,'yx')\n",
    "        if plot_true_preds:\n",
    "            a[i].plot(x,true_preds,'r+')\n",
    "            a[i].legend(['ground truth', 'quantized preds', 'real preds'], loc='upper left')\n",
    "        else:\n",
    "            a[i].legend(['ground truth', 'quantized preds'], loc='upper left')\n",
    "        combined_preds.append(true_preds_fixed)\n",
    "        temp2 = math.sqrt(np.mean(np.square(true_preds_fixed - y_test[:,i])))\n",
    "        print('RMSE for parameter ' + str(i+1) + ': '+ str(round(10000*temp2)/10000))\n",
    "    plt.show()\n",
    "    final_preds[algo] = combined_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "target_test_path = path_to_dataset_folder + '/DATA/Target_feature_test_CSVs'\n",
    "save_to_path = path_to_dataset_folder + '/final_results_base_learning'\n",
    "tree_names = os.listdir(tree_meta_feature_path + '/drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLM_gTruth_oneByOne(file, source, save_to_path, alpha, lam):\n",
    "    print('processing file: ' + file + ' | alpha:' + str(alpha) + ' | lambda:' + str(lam))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "           \n",
    "    model = h2o.estimators.glm.H2OGeneralizedLinearEstimator(model_id = 'GLMesti',\n",
    "                                                             seed = 888,\n",
    "                                                             alpha = alpha,\n",
    "                                                             lambda_ = lam,\n",
    "                                                             lambda_search = False,\n",
    "                                                             nlambdas = 100,\n",
    "                                                             standardize = True,\n",
    "                                                             nfolds = 10,\n",
    "                                                             keep_cross_validation_models = False)\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time\n",
    "    \n",
    "    lambda_param = float(model.summary()['regularization'][0].split('= ')[-1].split(' ')[0])\n",
    "    params = h2o.H2OFrame({'alpha':alpha,\n",
    "                           'lambda':lambda_param,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, save_to_path + '/GLM/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_alpha' + str(alpha) + '_lambda' + str(lambda_param) +'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_gTruth_oneByOne(file, source, save_to_path, ntrees, depth):\n",
    "    print('processing file: ' + file + ' | ntrees:' + str(ntrees) + ', Tree depth:' + str(depth))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time() #Start counting training time\n",
    "    \n",
    "    model = h2o.estimators.H2ORandomForestEstimator(model_id = 'RFesti',\n",
    "                                                    seed = 888,\n",
    "                                                    ntrees = ntrees,\n",
    "                                                    max_depth = depth,\n",
    "                                                    nfolds = 10,\n",
    "                                                    keep_cross_validation_models = False)\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time #Stop timing\n",
    "    \n",
    "    params = h2o.H2OFrame({'ntrees':ntrees,\n",
    "                           'max_depth':depth,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, save_to_path + '/RF/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_ntr' + str(ntrees) + '_dpth' + str(depth)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGboost_gTruth_oneByOne(file, source, save_to_path, ntrees, depth, colsample_bytree, method = 'cpu'):\n",
    "    print('processing file: ' + file + ' | ntrees:' + str(ntrees) + ', Tree depth:' + str(depth)+\n",
    "         ', colsPerTree:' + str(colsample_bytree))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = h2o.estimators.xgboost.H2OXGBoostEstimator(model_id = 'XGesti',\n",
    "                                                       seed = 888,\n",
    "                                                       backend= method,\n",
    "                                                       ntrees = ntrees,\n",
    "                                                       max_depth = depth,\n",
    "                                                       col_sample_rate_per_tree = colsample_bytree,\n",
    "                                                       nfolds = 10,\n",
    "                                                       keep_cross_validation_models = False)\n",
    "    print('Zoo')\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time\n",
    "    \n",
    "    params = h2o.H2OFrame({'ntrees':ntrees,\n",
    "                           'max_depth':depth,\n",
    "                           'cols_per_tree': colsample_bytree,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, save_to_path + '/XGboost/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_ntr' + str(ntrees) + '_dpth' + str(depth) + '_colsPerTree' + str(colsample_bytree) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init(max_mem_size = 64, nthreads = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for algor in ['GLM', 'RF', 'XGboost']:\n",
    "    params = []\n",
    "    if algor == 'XGboost':\n",
    "        param_num = 3\n",
    "    else:\n",
    "        param_num = 2\n",
    "    for i in range(param_num):\n",
    "        params.append(final_preds[algor][i])\n",
    "    flag = -1\n",
    "    for ind, file_name in enumerate(tree_names):\n",
    "        if ind in test_indices:\n",
    "            flag += 1\n",
    "            h2o.remove_all()\n",
    "            if algor == 'GLM':\n",
    "                GLM_gTruth_oneByOne(file_name, target_test_path, save_to_path, params[0][flag], params[1][flag])\n",
    "            elif algor == 'RF':\n",
    "                RF_gTruth_oneByOne(file_name, target_test_path, save_to_path, params[0][flag], params[1][flag])\n",
    "            elif algor == 'XGboost':\n",
    "                XGboost_gTruth_oneByOne(file_name, target_test_path, save_to_path, params[0][flag], params[1][flag], params[2][flag])\n",
    "            h2o.remove_all()\n",
    "            h2o.remove_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.remove_all()\n",
    "h2o.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate ground truth extraction time for the test set datasets.\n",
    "gTruth_rmses = {}\n",
    "final_rmses = {}\n",
    "for algor in ['GLM','RF', 'XGboost']:\n",
    "    temp_names = os.listdir(save_to_path + '/' + algor)\n",
    "    rmses_gTruth = []\n",
    "    rmses_final = []\n",
    "    method_time = 0\n",
    "    gTruthTime = 0\n",
    "    flag = -1\n",
    "    for ind,name in enumerate(tree_names):\n",
    "        if ind in test_indices:\n",
    "            flag += 1\n",
    "            df = pd.read_csv(ground_truth_path + '/' + algor + '/' + name, header = 0, sep = ',')\n",
    "            gTruthTime += df.iloc[0,-1]\n",
    "            rmses_gTruth.append(df.iloc[0,-2])\n",
    "            if algor == 'GLM':\n",
    "                df = pd.read_csv('C:Users/Dimiexe/Desktop' + '/' + algor + '/' + temp_names[flag], header = 0, sep = ',')\n",
    "            else:\n",
    "                df = pd.read_csv(save_to_path + '/' + algor + '/' + temp_names[flag], header = 0, sep = ',')\n",
    "            method_time += df.iloc[0,-1]\n",
    "            rmses_final.append(df.iloc[0,-2])\n",
    "    gTruth_rmses[algor] = rmses_gTruth\n",
    "    final_rmses[algor] = rmses_final\n",
    "    print(algor+' grid_time:',gTruthTime)\n",
    "    print(algor + ' method_time:',(method_time))\n",
    "    print('\\n', times[algor])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to save final predictions to  csv file, so as to have them available for later work.\n",
    "dddf = pd.DataFrame({'GLM_1':final_preds['GLM'][0], 'GLM_2':final_preds['GLM'][1],\n",
    "                    'RF_1':final_preds['RF'][0], 'RF_2':final_preds['RF'][1],\n",
    "                    'XGboost_1':final_preds['XGboost'][0],'XGboost_2':final_preds['XGboost'][1],'XGboost_3':final_preds['XGboost'][2]})\n",
    "dddf.to_csv('C:/Users/Dimiexe/Desktop/DATAsets/final_results_base_learning/final_preds.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

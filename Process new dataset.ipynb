{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target variable for base-learning is assumed to be the last column.\n",
    "#Can't automatically deal with date-type features.\n",
    "#Can't automatically reverse one hot encoding.\n",
    "#Can't automatically turn int-type encoding to enum-type encoding.\n",
    "#1st line in dataset is assumed to be header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import arff\n",
    "import re\n",
    "import codecs\n",
    "import sklearn.tree as tr\n",
    "from sklearn import preprocessing\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "import h2o\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Import data & Set algorithm ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Master path\n",
    "path_to_dataset_folder = '/home/User/Desktop/example_system'\n",
    "\n",
    "#Set algorithm\n",
    "algorithm = 'RF' #Options: 'GLM', 'RF', 'XGboost'.\n",
    "\n",
    "print_model_params = True #Used to print info of the used meta-models.\n",
    "\n",
    "#Set h2o cluster options.\n",
    "h2o_mem = 64\n",
    "h2o_threads =-1 #-1: max available cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_path = path_to_dataset_folder + '/test_dataset'\n",
    "first_nan_treat_path = path_to_dataset_folder + '/DATA/1st_NaN_Treatment'\n",
    "CSV_path = path_to_dataset_folder + '/DATA/CSVs'\n",
    "target_test_path = first_nan_treat_path\n",
    "tree_meta_feature_path = path_to_dataset_folder + '/DATA/tree_metafeatures_for_test_CSVs'\n",
    "manual_meta_feature_path = path_to_dataset_folder + '/DATA/manual_metafeatures_for_test_CSVs'\n",
    "metaModel_data_path = path_to_dataset_folder + '/DATA/metaModel_Datasets'\n",
    "model_path = path_to_dataset_folder + '/final_models'\n",
    "regularization_path = path_to_dataset_folder + '/regularization'\n",
    "save_to_path = path_to_dataset_folder + '/final_results_base_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Create paths ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meth in ['drop', 'mean', 'median']:\n",
    "    if not os.path.isdir(tree_meta_feature_path + '/' + meth):\n",
    "        os.makedirs(tree_meta_feature_path + '/' + meth)\n",
    "    if not os.path.isdir(manual_meta_feature_path + '/' + meth):\n",
    "        os.makedirs(manual_meta_feature_path + '/' + meth)\n",
    "for al in ['GLM', 'RF', 'XGboost']:\n",
    "    if not os.path.isdir(save_to_path + '/' + al):\n",
    "        os.makedirs(save_to_path + '/' + al)\n",
    "if not os.path.isdir(first_nan_treat_path):\n",
    "    os.mkdir(first_nan_treat_path)\n",
    "if not os.path.isdir(CSV_path):\n",
    "    os.mkdir(CSV_path)\n",
    "if not os.path.isdir(target_test_path):\n",
    "    os.mkdir(target_test_path)\n",
    "if not os.path.isdir(metaModel_data_path):\n",
    "    os.mkdir(metaModel_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Clear last Data ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_files(path):\n",
    "    for file in os.listdir(path):\n",
    "        os.remove(path + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,path in enumerate([tree_meta_feature_path, manual_meta_feature_path, save_to_path, metaModel_data_path,\n",
    "                         CSV_path, first_nan_treat_path]):\n",
    "    if i in [0,1]:\n",
    "        for method in ['drop', 'mean', 'median']:\n",
    "            clear_files(path + '/' + method)\n",
    "    elif i == 2:\n",
    "        for al in ['GLM', 'RF', 'XGboost']:\n",
    "            clear_files(path + '/' + al)\n",
    "    else:\n",
    "        clear_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Preprocessing ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing functions\n",
    "def checkTargetDir(full_name):\n",
    "    writeDir = ''.join('/'.join(full_name.split('/')[0:-2]) + '/DATA/CSVs')\n",
    "    if not os.path.isdir(writeDir):\n",
    "        os.mkdir(writeDir)\n",
    "    return writeDir\n",
    "\n",
    "def tabAndWhite(full_name):\n",
    "    print('Dealing with tabs and whitespaces in: ' + full_name.split('/')[-1])\n",
    "    with codecs.open(full_name, 'r', 'utf-8') as curFile:\n",
    "        data = curFile.readlines()\n",
    "    data = [re.sub('\\t', ' ',x.strip()) for x in data]    #Turning tabs to whitespace,stripping start/end.\n",
    "    data = [re.sub(' +',' ',x) for x in data]    #Turning multiple whitespaces to length of one.\n",
    "    data = '\\n'.join(data)\n",
    "    tempFile = ''.join('/'.join(full_name.split('/')[0:-1]) + '/v2_' + full_name.split('/')[-1])\n",
    "    with codecs.open(tempFile, 'w', 'utf-8') as curFile:\n",
    "        for row in data:\n",
    "            curFile.write(row)\n",
    "    return tempFile\n",
    "\n",
    "def process_csv_txt_data(original_name):\n",
    "    filename = original_name.split('/')[-1]\n",
    "    print('Processing ' + filename)\n",
    "    full_name = tabAndWhite(original_name)\n",
    "    file_size = os.stat(full_name).st_size\n",
    "    with codecs.open(full_name, 'r', 'utf-8') as curFile:\n",
    "        df = pd.read_table(curFile,header = 0, sep = None)\n",
    "    os.remove(full_name)\n",
    "    writeDir = checkTargetDir(full_name)\n",
    "    df.to_csv(os.path.join(writeDir,'.'.join(filename.split('.')[0:-1]) +'.csv'),index =False)\n",
    "    print('----------------------------------------------------------------------')\n",
    "    \n",
    "def process_excel(full_name):\n",
    "    filename = full_name.split('/')[-1]\n",
    "    print('Processing ' + filename)\n",
    "    df = pd.read_excel(full_name)\n",
    "    writeDir = checkTargetDir(full_name)\n",
    "    df.to_csv(os.path.join(writeDir,'.'.join(filename.split('.')[0:-1]) +'.csv'),index =False)\n",
    "    print('----------------------------------------------------------------------')\n",
    "\n",
    "def process_arff(full_name):\n",
    "    print('Processing ' + full_name.split('/')[-1])\n",
    "    with codecs.open(full_name,encoding = 'utf-8') as curFile:\n",
    "        df = arff.load(curFile)\n",
    "    df = pd.DataFrame(df['data'])\n",
    "    writeDir = checkTargetDir(full_name)\n",
    "    df.to_csv(os.path.join(writeDir,'.'.join(filename.split('.')[0:-1]) +'.csv'),index =False)\n",
    "    print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "#Creating \"files\" as a one time assignment of listdir so as to\n",
    "#avoid processing later-created-temp files as well.\n",
    "files = os.listdir(new_dataset_path)\n",
    "for filename in files:\n",
    "    full_name = new_dataset_path +'/' + filename\n",
    "    suf = filename.split('.')[-1]\n",
    "    if suf in [\"txt\",\"csv\",\"data\",\"dat\"]:\n",
    "        process_csv_txt_data(full_name)\n",
    "    elif suf in [\"xls\", \"xlsx\"]:\n",
    "        process_excel(full_name)\n",
    "    elif suf == \"arff\":\n",
    "        process_arff(full_name)\n",
    "    else:\n",
    "        print('Unsupported file: ' + filename + ' ...ignoring...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesWithNan(df):\n",
    "    nansPerLine = df.isna().sum(axis=1)\n",
    "    lines_with_nan = 0\n",
    "    for l in nansPerLine:\n",
    "        if l>0:\n",
    "            lines_with_nan +=1\n",
    "    return lines_with_nan\n",
    "\n",
    "#Used to perform the 1st NaN treatment of the dataset. Any Feature with more than 20% missing\n",
    "#values is discarded. After all such features have been discarded if the lines with at least one\n",
    "#missing vallue are less than 10% of the total remaining lines, they are droped. The resulting\n",
    "#datasets are saved in \"1st_NaN_Treatment\" folder of the \"../DATAsets/DATA\" dir.\n",
    "def first_nan_treatment(path,exclusions = None, feature_threshold = 0.20,\n",
    "                        lines_with_nan_threshold = 0.10, visual_info = True):\n",
    "    files = os.listdir(path)\n",
    "    for f in np.setdiff1d(files, exclusions,assume_unique=True):\n",
    "        df = pd.read_table(path + '/' + f, header = 0, sep = ',')\n",
    "        lines = df.shape[0]\n",
    "        features = df.shape[1]\n",
    "                \n",
    "        #Nans on Feature basis\n",
    "        nanPerFeature = df.isna().sum(axis=0)\n",
    "        indecies_to_drop = []\n",
    "        for i,npf in enumerate(nanPerFeature):\n",
    "            if npf / lines >= feature_threshold:\n",
    "                indecies_to_drop.append(i)\n",
    "        df.drop(df.columns[indecies_to_drop],axis=1,inplace=True)\n",
    "        \n",
    "        #Nans on Line basis\n",
    "        nansPerLine = df.isna().sum(axis=1)\n",
    "        lines_with_nan = linesWithNan(df)\n",
    "        if (lines_with_nan / lines) <= lines_with_nan_threshold:\n",
    "            df.dropna(axis = 0, how='any',inplace=True)\n",
    "        new_lines_with_nan = linesWithNan(df)\n",
    "        if visual_info:\n",
    "            if features != df.shape[1] or lines != df.shape[0] or new_lines_with_nan/df.shape[0] >=lines_with_nan_threshold:\n",
    "                print('File:' + f)\n",
    "                print('Lines:' + str(lines) + ' --> ' +str(df.shape[0]))\n",
    "                print('Features: ' +str(features) + ' --> '+str(df.shape[1]))\n",
    "                print('Lines with NaNs: ' + str(lines_with_nan) + ' ('+\n",
    "                      str(round(lines_with_nan / lines * 10000)/100) + '%) --> ' +\n",
    "                      str(new_lines_with_nan)+ '('+ str(round(new_lines_with_nan /\n",
    "                                                              df.shape[0] * 10000)/100) + '%)')\n",
    "                print('NaNs per Feature:\\n' + str(df.isna().sum(axis=0)))\n",
    "                print('---------------------------------------------')\n",
    "        df.to_csv(first_nan_treat_path + '/' + f, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the NaN treatment\n",
    "first_nan_treatment(CSV_path, visual_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dataset after 1st nan treatment and drop any constant columns,\n",
    "#while preserving the naming of the remaining columns.\n",
    "for file in os.listdir(first_nan_treat_path):\n",
    "    df = pd.read_table(first_nan_treat_path + '/'+ file,sep=',')\n",
    "    L = df.shape[1]\n",
    "    drops = []\n",
    "    i=0\n",
    "    for feat in df.columns:\n",
    "        if len(df.loc[:,feat].unique()) == 1:\n",
    "            if i == 0:\n",
    "                print('Processing file: ' + file)\n",
    "                print(df.head())\n",
    "            print('Found one! It is column: ' + feat)\n",
    "            print('Unique value = ', df.loc[0,feat])\n",
    "            drops.append(feat)\n",
    "            L -=1\n",
    "            i+=1\n",
    "    flag = False\n",
    "    if df.columns[0] == '0':\n",
    "        flag = True\n",
    "    df.drop(drops, axis = 1, inplace = True)\n",
    "    if flag:\n",
    "        df.columns = range(L)\n",
    "    if len(drops)>0:\n",
    "        print(df.head())\n",
    "    df.to_csv(first_nan_treat_path + '/'+ file,sep=',',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Extraction of Metafeatures ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Tree metafeatures ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to make a dictionary with leave nodes' ids maped to their branch lengths. So there are that many key:value\n",
    "#entries as the numbeer of leaves. The aformentioned dictionary is the global variable \"branch_lengths\".\n",
    "branch_lengths = {}\n",
    "def get_branches_len(input_tree_, start_node, counter):\n",
    "    if input_tree_.children_left[start_node] !=-1:\n",
    "        counter += 1\n",
    "        get_branches_len(input_tree_,input_tree_.children_left[start_node],counter)\n",
    "        get_branches_len(input_tree_,input_tree_.children_right[start_node],counter)\n",
    "    else:\n",
    "        global branch_lengths\n",
    "        branch_lengths[start_node] = counter\n",
    "        \n",
    "#Returns a list of ints, containing the node ids of the leaves.\n",
    "def find_leaves(input_tree_):\n",
    "    leaves = []\n",
    "    for node in range(input_tree_.node_count):\n",
    "        if input_tree_.children_left[node] == -1:\n",
    "            leaves.append(node)\n",
    "    return leaves\n",
    "\n",
    "#Returns a dictionary with level-id int keys maped to the nodes on that level. e.g. 0:0, 1:[1,2] for a simple 2-level tree\n",
    "#root + 2 children nodes.\n",
    "def get_levels(input_tree_):\n",
    "    levels = {0:[0]}\n",
    "    for i in range(input_tree_.max_depth):\n",
    "        left_children = [input_tree_.children_left[x] for x in levels[i] if input_tree_.children_left[x] != -1]\n",
    "        right_children = [input_tree_.children_right[x] for x in levels[i] if input_tree_.children_right[x] != -1]\n",
    "        levels[i+1] = left_children + right_children\n",
    "    return levels\n",
    "\n",
    "#Returns a dictionary with the features as keys, and their frequency of appearance as values.\n",
    "def get_feature_freq(input_tree_):\n",
    "    freqs = {}\n",
    "    for i in range(input_tree_.node_count):\n",
    "        if input_tree_.children_left[i] !=-1:\n",
    "            if input_tree_.feature[i] in freqs:\n",
    "                freqs[input_tree_.feature[i]] +=1\n",
    "            else:\n",
    "                freqs[input_tree_.feature[i]] = 1\n",
    "    return freqs\n",
    "\n",
    "#The next 2 functions' code was found at: https://www.geeksforgeeks.org/diameter-of-a-binary-tree-in-on-a-new-method/\n",
    "#and tweaked slightly to match the needs of this work.\n",
    "\n",
    "# Function to find height of a tree  \n",
    "def height(tree_,root, ans): \n",
    "    if (root == -1): \n",
    "        return 0\n",
    "  \n",
    "    left_height = height(tree_,tree_.children_left[root], ans)  \n",
    "  \n",
    "    right_height = height(tree_,tree_.children_right[root], ans)  \n",
    "  \n",
    "    # update the answer, because diameter  \n",
    "    # of a tree is nothing but maximum  \n",
    "    # value of (left_height + right_height + 1) \n",
    "    # for each node  \n",
    "    ans[0] = max(ans[0], 1 + left_height + \n",
    "                             right_height)  \n",
    "  \n",
    "    return 1 + max(left_height, \n",
    "                   right_height) \n",
    "  \n",
    "# Computes the diameter of binary  \n",
    "# tree with given root.  \n",
    "def diameter(tree_,root): \n",
    "    if (root < 0):  \n",
    "        return 0\n",
    "    ans = [-999999999999] # This will store \n",
    "                          # the final answer  \n",
    "    height_of_tree = height(tree_,root, ans)  \n",
    "    return ans[0]\n",
    "\n",
    "#Returns the 14-element feature vector (list) of the given dataFrame (df). Also if \"visual_tree\" is True,\n",
    "#then the tree is ploted, default value: False.\n",
    "def metafeatures(df, visual_tree = False):\n",
    "    meta_vector = []\n",
    "    x = df.loc[:,df.columns[0:-1]]\n",
    "    y = df.loc[:,df.columns[-1]]\n",
    "    regr = tr.DecisionTreeRegressor(random_state = 8328, min_impurity_decrease = 1e-06)\n",
    "    regr.fit(x,y)\n",
    "    if visual_tree:\n",
    "        tr.plot_tree(regr)\n",
    "    \n",
    "    #1 Tree width (diameter)\n",
    "    meta_vector.append(diameter(regr.tree_,0))    \n",
    "    #2 Tree height\n",
    "    meta_vector.append(regr.tree_.max_depth)    \n",
    "    #3 Total number of nodes\n",
    "    meta_vector.append(regr.tree_.node_count)    \n",
    "    #4 Total number of leaves\n",
    "    meta_vector.append(len(find_leaves(regr.tree_)))\n",
    "    \n",
    "    levels = get_levels(regr.tree_)\n",
    "    nodes_per_level = [len(levels[i]) for i in range(len(levels))]    \n",
    "    #5 Maximun nodes per level\n",
    "    meta_vector.append(np.max(nodes_per_level))\n",
    "    #6 Mean number of nodes per level\n",
    "    meta_vector.append(np.mean(nodes_per_level))\n",
    "    #7 Standard deviation of nodes per level\n",
    "    meta_vector.append(np.std(nodes_per_level))\n",
    "    \n",
    "    global branch_lengths\n",
    "    branch_lengths = {} #(re)initialise the global variable to hold the branches and their lengths\n",
    "    get_branches_len(regr.tree_,0,0)\n",
    "    length_per_branch = [branch_lengths[x] for x in branch_lengths]\n",
    "    #(8) Longest branch's length| Not used because it's always the same as #2 Tree height\n",
    "    #meta_vector.append(np.max(length_per_branch))\n",
    "    #8 Shortest branch's length\n",
    "    meta_vector.append(np.min(length_per_branch))\n",
    "    #9 Mean length of branches\n",
    "    meta_vector.append(np.mean(length_per_branch))\n",
    "    #10 Standard deviation of length of branches\n",
    "    meta_vector.append(np.std(length_per_branch))\n",
    "    \n",
    "    feature_frequencies = get_feature_freq(regr.tree_)\n",
    "    freqs = [feature_frequencies[x] for x in feature_frequencies]\n",
    "    #11 Maximum frequency of feature appearance\n",
    "    meta_vector.append(np.max(freqs))\n",
    "    #12 Minimum frequency of feature appearance\n",
    "    meta_vector.append(np.min(freqs))\n",
    "    #13 Mean frequency of feature appearance\n",
    "    meta_vector.append(np.mean(freqs))\n",
    "    #14 Standard deviation of frequency of feature appearance\n",
    "    meta_vector.append(np.std(freqs))\n",
    "    \n",
    "    return meta_vector\n",
    "\n",
    "#This function prepares the dataframe for metafeature extraction. \n",
    "#First a missing values imputation is performed using the \"method\" arguement:\n",
    "# drop = all instances with at least one missing value are droped,\n",
    "# mean = missing values are assigned the mean value for each numerical feature,\n",
    "# median = missing values are assigned the median value for each numerical feature.\n",
    "#In any case, any missing values on categorical deatures are filled with the mode of\n",
    "#each feature, the value most frequently seen.\n",
    "#Returns the processed dataframe and the NaNs per line metric.\n",
    "def tree_ready(df, method = 'drop'):\n",
    "    total_nans = df.isna().sum().sum()\n",
    "    mean_nans_per_line = total_nans / df.shape[0]\n",
    "    feats_to_encode = [feat for i, feat in enumerate(df.columns) if df.dtypes[i] == 'object']\n",
    "    if total_nans > 0:\n",
    "        cols = np.setdiff1d(df.columns, feats_to_encode)\n",
    "        if method == 'drop':\n",
    "            df.dropna(axis = 0, how = 'any', inplace = True)\n",
    "        elif method == 'mean':            \n",
    "            df.fillna({col: df.loc[:,col].mean() for col in cols}, inplace = True)\n",
    "            df.fillna({col: df.loc[:,col].mode()[0] for col in feats_to_encode}, inplace = True)\n",
    "        elif method == 'median':\n",
    "            df.fillna({col: df.loc[:,col].median() for col in cols}, inplace = True)\n",
    "            df.fillna({col: df.loc[:,col].mode()[0] for col in feats_to_encode}, inplace = True)\n",
    "        else:\n",
    "            print('Non identifiable method provided!')\n",
    "            return None, None\n",
    "        \n",
    "    for x in feats_to_encode:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        df.loc[:,x] = le.fit_transform(df.loc[:,x])\n",
    "    return df , mean_nans_per_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates and processes the tree to get the metafatures. The metafeature vector is saved in\n",
    "#a txt file located in \"../DATAsets/DATA/tree_metafeatures_for_test_CSVs\" dir.\n",
    "datasets = os.listdir(target_test_path)\n",
    "start_time = time.time()\n",
    "for file in datasets:\n",
    "    print('proccessing file: ' + file)\n",
    "    print('Algorithm:',algorithm)\n",
    "    for method in ['drop','mean','median']:\n",
    "        if not os.path.isfile(tree_meta_feature_path + '/' + method + '/' + file):\n",
    "            print('method: ' + method)\n",
    "            df = pd.read_table(target_test_path + '/'+ file,sep=',')\n",
    "            df, MNpL = tree_ready(df, method)\n",
    "\n",
    "            with open(tree_meta_feature_path + '/' + method + '/' + file,'w' ) as f:\n",
    "                for i,x in enumerate(metafeatures(df)):\n",
    "                    f.write(str(x)+',')\n",
    "                f.write(str(MNpL))\n",
    "mfe_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Manual metafeatures ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitions of functions for >>>MANUAL<<< computation of general and statistical\n",
    "#type metafeatures of the given dataset.\n",
    "\n",
    "def append_mean_and_std(vec, values):\n",
    "    vec.append(np.mean(values))\n",
    "    vec.append(np.std(values))\n",
    "\n",
    "def manual_mfe_extraction(data):    \n",
    "    meta_vector = []\n",
    "    x = df.loc[:,df.columns[0:-1]]\n",
    "    y = df.loc[:,df.columns[-1]]\n",
    "    \n",
    "    # ------------ General metafeatures ------------\n",
    "    # 1. number of instances\n",
    "    nr_inst = x.shape[0]\n",
    "    meta_vector.append(nr_inst)\n",
    "    # 2. number of features\n",
    "    nr_attr = x.shape[1]\n",
    "    meta_vector.append(nr_attr)\n",
    "    # 3. ratio between instances and features\n",
    "    inst_to_attr = nr_inst / nr_attr\n",
    "    meta_vector.append(inst_to_attr)\n",
    "    # 4. number of categorical features\n",
    "    nr_cat = 0\n",
    "    # 5. number of numeric features\n",
    "    nr_num = 0\n",
    "    \n",
    "    uniques_values_per_feature = x.apply(pd.Series.nunique)\n",
    "    classes = []\n",
    "    for i, feature in enumerate(x.columns):\n",
    "        if x.dtypes[i] == 'object':\n",
    "            nr_cat +=1\n",
    "            classes.append(uniques_values_per_feature[i])\n",
    "        else:\n",
    "            nr_num +=1\n",
    "    meta_vector.append(nr_cat)\n",
    "    meta_vector.append(nr_num)\n",
    "    # 6. categorical to numerical ratio\n",
    "    cat_to_num = nr_cat / nr_num\n",
    "    meta_vector.append(cat_to_num)\n",
    "    # 7. & 8. number of distinct classes (mean and std)\n",
    "    if len(classes) == 0:\n",
    "        classes = [0]\n",
    "    append_mean_and_std(meta_vector, classes)\n",
    "    \n",
    "    # ------------ Statistical metafeatures ------------\n",
    "    cor_matrix = x.corr(method = 'pearson').abs()\n",
    "    cov_matrix = x.cov().abs()\n",
    "    cor_flat = []\n",
    "    cov_flat = []\n",
    "    for i in range(cor_matrix.shape[0]):\n",
    "        for j in range(cor_matrix.shape[1]):\n",
    "            if j > i:\n",
    "                cor_flat.append(cor_matrix.iloc[i,j])\n",
    "                cov_flat.append(cov_matrix.iloc[i,j])\n",
    "    if len(cor_flat) == 0: #if there are less than 2 numeric features in dataset.\n",
    "        cor_flat = [0]\n",
    "        cov_flat = [0]\n",
    "    # 9. & 10. absolute pairwise correlations (mean and std)\n",
    "    append_mean_and_std(meta_vector, cor_flat)\n",
    "    # 11. & 12. absolute pairwise covariances (mean and std)\n",
    "    append_mean_and_std(meta_vector, cov_flat)\n",
    "    # 13. & 14. kurtosis of features (mean and std)\n",
    "    kurt = x.kurtosis(axis = 0, numeric_only = True)\n",
    "    append_mean_and_std(meta_vector, kurt)\n",
    "    # 15. & 16. mean absolute deviation of features (mean and std)\n",
    "    mad = x.mad(axis = 0)\n",
    "    append_mean_and_std(meta_vector, mad)\n",
    "    # 17. & 18. maximum value of features (mean and std)\n",
    "    maxes = x.max(axis = 0, numeric_only = True)\n",
    "    append_mean_and_std(meta_vector, maxes)\n",
    "    # 19. & 20. mean value of features (mean and std)\n",
    "    means = x.mean(axis = 0, numeric_only = True)\n",
    "    append_mean_and_std(meta_vector, means)\n",
    "    # 21. & 22. median of features (mean and std)\n",
    "    medians = x.median(axis = 0, numeric_only = True)\n",
    "    append_mean_and_std(meta_vector, medians)\n",
    "    # 23. & 24. minimum value of features (mean and std)\n",
    "    mins = x.min(axis = 0, numeric_only = True)\n",
    "    append_mean_and_std(meta_vector, mins)\n",
    "    # 25. & 26. standard deviation of value of features (mean and std)\n",
    "    stds = x.std(axis = 0, numeric_only = True)\n",
    "    append_mean_and_std(meta_vector, stds)\n",
    "    # 27. & 28. skewness of features (mean and std)\n",
    "    skews = x.skew(axis = 0, numeric_only = True)\n",
    "    append_mean_and_std(meta_vector, skews)\n",
    "    # 29. & 30. variance of features (mean and std)\n",
    "    variances = x.var(axis = 0, numeric_only = True)\n",
    "    append_mean_and_std(meta_vector, variances)\n",
    "    \n",
    "    return meta_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The metafeature vector is saved in  txt file located in \n",
    "# \"../DATAsets/DATA/metafeatures_for_test_CSVs\" dir.\n",
    "datasets = os.listdir(target_test_path)\n",
    "start_time = time.time()\n",
    "for file in datasets:\n",
    "    print('proccessing file: ' + file)\n",
    "    for method in ['drop','mean','median']:\n",
    "        if not os.path.isfile(manual_meta_feature_path + '/' + method + '/' + file):\n",
    "            print('method: ' + method)\n",
    "            df = pd.read_table(target_test_path + '/'+ file,sep=',')\n",
    "            metafeatures = manual_mfe_extraction(df)\n",
    "\n",
    "            with open(manual_meta_feature_path + '/' + method + '/' + file,'w' ) as f:\n",
    "                for i,x in enumerate(metafeatures):\n",
    "                    if i < len(metafeatures) - 1:\n",
    "                        f.write(str(x)+',')\n",
    "                    else:\n",
    "                        f.write(str(x))\n",
    "mfe_time = mfe_time + time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Meta learning process ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict_RF(x_test, y_cols, estimator):\n",
    "    predictions = []\n",
    "    for target_column in range(y_cols):\n",
    "        y_pred = estimator.predict(x_test)\n",
    "        predictions.append(y_pred)\n",
    "    return predictions\n",
    "\n",
    "#Combines the extracted metafeatures and ground truth of the dataset into a new dataset.\n",
    "#It contains the 16 metafeatures + 2 (3 for xgboost) ground truth hyperparameters.\n",
    "\n",
    "def build_input_vec(tree_meta_feature_path, manual_meta_feature_path = None,\n",
    "                     algo = 'RF', method = 'drop', eco = True):\n",
    "    for f in os.listdir(tree_meta_feature_path + '/' + method):\n",
    "        line = pd.read_csv(tree_meta_feature_path + '/' + method + '/' + f,\n",
    "                           names = ['TreeDiam', 'TreeHeight', 'TotalNodes', 'TotalLeaves',\n",
    "                                    'maxNodePerLevel', 'meanNodePerLevel', 'stdNodePerLevel',\n",
    "                                    'ShortBranch', 'meanBranch', 'stdBranch','maxFeatureFreq',\n",
    "                                    'minFeatureFreq', 'meanFeatureFreq', 'stdFeatureFreq','NaNsPerLine'\n",
    "                                   ],\n",
    "                           header = None, sep=',')\n",
    "        if not manual_meta_feature_path == None:\n",
    "            line2 = pd.read_csv(manual_meta_feature_path + '/' + method + '/' + f,\n",
    "                                names = ['nr_inst', 'nr_attr', 'inst_to_attr', 'nr_cat', 'nr_num', 'cat_to_num',\n",
    "                                         'nr_class_mean', 'nr_class_std', 'cor_mean', 'cor_std', 'cov_mean', 'cov_std',\n",
    "                                         'kurtosis_mean', 'kurtosis_std', 'mad_mean', 'mad_std', 'max_mean', 'max_std',\n",
    "                                         'mean_mean', 'mean_std', 'median_mean', 'median_std', 'min_mean', 'min_std',\n",
    "                                         'std_mean', 'std_std', 'skew_mean', 'skew_std','var_mean', 'var_std'\n",
    "                                        ],\n",
    "                                header = None, sep=',')\n",
    "            line = pd.concat([line, line2], ignore_index = False, axis = 1)\n",
    "    line.to_csv(metaModel_data_path + '/' + algo + '_' + method + '.csv', sep =',', index = False)\n",
    "    return line\n",
    "\n",
    "def regularize_input(data, algo, method, regularization_path):\n",
    "    regularization_means = pd.read_csv(regularization_path + '/regularization_means_' + algo + '_' + method + '.csv')\n",
    "    regularization_stds = pd.read_csv(regularization_path + '/regularization_stds_' + algo + '_' + method + '.csv')\n",
    "    L = data.columns\n",
    "    for j in L:\n",
    "        for i,x in enumerate(data.loc[:,j]):\n",
    "            data.loc[i,j] = (x-regularization_means.loc[0,j])/regularization_stds.loc[0,j]\n",
    "    return data.values\n",
    "\n",
    "def fix_preds_calcs(param_vector, single_pred):\n",
    "    param_vector2 = [abs(x - single_pred) for x in param_vector]\n",
    "    temp = 0   #Store parameter value\n",
    "    ind = 100000  #Store parameter comparison value\n",
    "    for i,pa in enumerate(param_vector2):\n",
    "        if pa < ind:\n",
    "            ind = pa\n",
    "            temp = param_vector[i]\n",
    "    return temp\n",
    "\n",
    "def fix_preds_RF(preds, algo, index, lambdas):\n",
    "    in_preds = []\n",
    "    length = len(preds)\n",
    "    if algo == 'GLM':\n",
    "        if index == 0:\n",
    "            y = range(0, 1100, 125)\n",
    "            param_alpha = [x / 1000 for x in y]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_alpha, preds[i]))\n",
    "        elif index == 1:\n",
    "            for i in range(length):\n",
    "                in_preds.append(preds[i])\n",
    "                \n",
    "    elif algo =='RF':\n",
    "        if index == 0:\n",
    "            param_tree = [25,50,75,100,200,300,400,500]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_tree, preds[i]))\n",
    "        elif index == 1:\n",
    "            param_depth = [20,40,60,80]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_depth, preds[i]))\n",
    "    elif algo == 'XGboost':\n",
    "        if index == 0:\n",
    "            param_tree = [25,50,100,200]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_tree, preds[i]))\n",
    "        elif index == 1:\n",
    "            param_depth = [6,10,15]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(param_depth, preds[i]))\n",
    "        elif index == 2:\n",
    "            colsample_bytree = [.6,.7,.8,.9]\n",
    "            for i in range(length):\n",
    "                in_preds.append(fix_preds_calcs(colsample_bytree, preds[i]))\n",
    "    return in_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Load meta-models\n",
    "RF_models = {}\n",
    "for i in os.listdir(model_path):\n",
    "    model_name = i.split('/')[-1].split('.')[0]\n",
    "    prefix = model_name.split('_')[0]\n",
    "    algo = model_name.split('_')[2]\n",
    "    if algo == algorithm:\n",
    "        if prefix == 'RF':\n",
    "            param = model_name.split('_')[-1]\n",
    "            key = algo + '_' + param\n",
    "            RF_models[key] = joblib.load(model_path + '/'+ i)\n",
    "        else:\n",
    "            NN_model = load_model(model_path + '/'+ i)\n",
    "\n",
    "#Prepare input data for RF-type meta-models\n",
    "if algorithm == 'GLM':\n",
    "    meta = ['all', 'tree']\n",
    "    method = ['drop', 'median']\n",
    "elif algorithm == 'RF':\n",
    "    meta = ['tree', 'all']\n",
    "    method = ['drop', 'drop']\n",
    "elif algorithm == 'XGboost':\n",
    "    meta = ['all', 'all', 'all']\n",
    "    method = ['mean', 'drop', 'drop']\n",
    "    print('Algorithm:',algorithm)\n",
    "\n",
    "#Do predictions for RF-type meta-models\n",
    "RF_preds = []\n",
    "print('----- RF predictions -----')\n",
    "for i in range(len(method)):\n",
    "    if print_model_params:\n",
    "        print('NaN method: ' + method[i])\n",
    "    if meta[i] == 'all':\n",
    "        if print_model_params:\n",
    "            print('Meta-features used: 45')\n",
    "        inp_vector = build_input_vec(tree_meta_feature_path,\n",
    "                                     manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                     algo = algorithm, method = method[i]\n",
    "                                    )            \n",
    "    elif meta[i] == 'tree':\n",
    "        if print_model_params:\n",
    "            print('Meta-features used: 15')\n",
    "        inp_vector = build_input_vec(tree_meta_feature_path,\n",
    "                                     algo = algorithm, method = method[i]\n",
    "                                    )\n",
    "            \n",
    "    model_key = algorithm + '_param' + str(i+1)\n",
    "    batch_preds = do_predict_RF(inp_vector, len(method), RF_models[model_key])\n",
    "    RF_preds.append(batch_preds[i][0])\n",
    "print('RF results:',RF_preds)\n",
    "print('\\n')\n",
    "        \n",
    "#Do predictions for NN-type meta-models\n",
    "print('----- NN predictions -----')\n",
    "#Print model info\n",
    "standardize = 'regularize'\n",
    "if algorithm == 'GLM':\n",
    "    meta = 'tree'\n",
    "    standardize_y = False\n",
    "    two_layers = False\n",
    "    half_on_second = False\n",
    "    method = 'drop'\n",
    "    hidden = 5\n",
    "elif algorithm == 'RF':\n",
    "    meta = 'tree'\n",
    "    standardize_y = False\n",
    "    two_layers = True\n",
    "    half_on_second = False\n",
    "    method = 'mean'\n",
    "    hidden = 10\n",
    "elif algorithm =='XGboost':\n",
    "    meta = 'tree'\n",
    "    standardize_y = False\n",
    "    two_layers = False\n",
    "    method = 'drop'\n",
    "    hidden = 5        \n",
    "\n",
    "if print_model_params:\n",
    "    print('Metafeature set:',meta)\n",
    "    print('Standardization:',standardize)\n",
    "    print('Hidden neurons:', hidden)\n",
    "    print('2nd hidden layer:',two_layers)\n",
    "    print('Nan method:',method)\n",
    "    \n",
    "#Prepare input data\n",
    "if meta == 'all':\n",
    "    inp_vector = build_input_vec(tree_meta_feature_path,\n",
    "                                 manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                 algo = algorithm, method = method\n",
    "                                )\n",
    "elif meta == 'tree':\n",
    "    inp_vector = build_input_vec(tree_meta_feature_path,\n",
    "                                 algo = algorithm, method = method\n",
    "                                )\n",
    "reg_inp_vector = regularize_input(inp_vector, algorithm, method, regularization_path)\n",
    "NN_preds = NN_model.predict(reg_inp_vector, verbose = 0)[0]\n",
    "NN_preds = NN_preds.tolist()\n",
    "print('NN results:', NN_preds)\n",
    "print('\\n')\n",
    "\n",
    "#Combine predictions\n",
    "print('----- Combined predictions -----')\n",
    "RF_combine_weight = 0.5\n",
    "NN_combine_weight = 0.5\n",
    "y_cols = len(NN_preds)\n",
    "combined_preds = []\n",
    "print('Combined predictions:')\n",
    "for i in range(y_cols):\n",
    "    true_preds = RF_preds[i] * RF_combine_weight + NN_preds[i] * NN_combine_weight\n",
    "    if algorithm == 'GLM' and i == 1:\n",
    "        true_preds_fixed = [true_preds]\n",
    "    else:\n",
    "        temp = []\n",
    "        temp.append(true_preds)\n",
    "        true_preds_fixed = fix_preds_RF(temp, algorithm, i, lambdas = None)\n",
    "    combined_preds.append(true_preds_fixed[0])\n",
    "print('Combined preds:',combined_preds)\n",
    "\n",
    "pred_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLM_gTruth_oneByOne(file, source, save_to_path, alpha, lam):\n",
    "    print('processing file: ' + file + ' | alpha:' + str(alpha) + ' | lambda:' + str(lam))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "           \n",
    "    model = h2o.estimators.glm.H2OGeneralizedLinearEstimator(model_id = 'GLMestimator',\n",
    "                                                             seed = 888,\n",
    "                                                             alpha = alpha,\n",
    "                                                             lambda_ = lam,\n",
    "                                                             lambda_search = False,\n",
    "                                                             nlambdas = 100,\n",
    "                                                             standardize = True,\n",
    "                                                             nfolds = 10,\n",
    "                                                             keep_cross_validation_models = False)\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time\n",
    "    \n",
    "    lambda_param = float(model.summary()['regularization'][0].split('= ')[-1].split(' ')[0])\n",
    "    params = h2o.H2OFrame({'alpha':alpha,\n",
    "                           'lambda':lambda_param,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, save_to_path + '/GLM/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_alpha' + str(alpha) + '_lambda' + str(lambda_param) +'.csv')\n",
    "    my_mojo = model.download_mojo(path = save_to_path + '/GLM/', get_genmodel_jar = True)\n",
    "\n",
    "def RF_gTruth_oneByOne(file, source, save_to_path, ntrees, depth):\n",
    "    print('processing file: ' + file + ' | ntrees:' + str(ntrees) + ', Tree depth:' + str(depth))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time() #Start counting training time\n",
    "    \n",
    "    model = h2o.estimators.H2ORandomForestEstimator(model_id = 'RFestimator',\n",
    "                                                    seed = 888,\n",
    "                                                    ntrees = ntrees,\n",
    "                                                    max_depth = depth,\n",
    "                                                    nfolds = 10,\n",
    "                                                    keep_cross_validation_models = False)\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time #Stop timing\n",
    "    \n",
    "    params = h2o.H2OFrame({'ntrees':ntrees,\n",
    "                           'max_depth':depth,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, save_to_path + '/RF/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_ntr' + str(ntrees) + '_dpth' + str(depth)+'.csv')\n",
    "    my_mojo = model.download_mojo(path = save_to_path + '/RF/', get_genmodel_jar = True)\n",
    "\n",
    "def XGboost_gTruth_oneByOne(file, source, save_to_path, ntrees, depth, colsample_bytree, method = 'cpu'):\n",
    "    print('processing file: ' + file + ' | ntrees:' + str(ntrees) + ', Tree depth:' + str(depth)+\n",
    "         ', colsPerTree:' + str(colsample_bytree))\n",
    "    data = h2o.import_file(source + '/' + file, header = 1)\n",
    "    y = data.columns[-1]\n",
    "    x = data.columns[0:-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = h2o.estimators.xgboost.H2OXGBoostEstimator(model_id = 'XGboostEstimator',\n",
    "                                                       seed = 888,\n",
    "                                                       backend= method,\n",
    "                                                       ntrees = ntrees,\n",
    "                                                       max_depth = depth,\n",
    "                                                       col_sample_rate_per_tree = colsample_bytree,\n",
    "                                                       nfolds = 10,\n",
    "                                                       keep_cross_validation_models = False)\n",
    "    model.train(x=x, y=y, training_frame=data)\n",
    "    \n",
    "    start_time = time.time() - start_time\n",
    "    \n",
    "    params = h2o.H2OFrame({'ntrees':ntrees,\n",
    "                           'max_depth':depth,\n",
    "                           'cols_per_tree': colsample_bytree,\n",
    "                           'rmse': model.rmse(xval=True),\n",
    "                           'calc_time':start_time\n",
    "                          })\n",
    "    h2o.download_csv(params, save_to_path + '/XGboost/' + '.'.join(file.split('.')[0:-1]) + \n",
    "                    '_ntr' + str(ntrees) + '_dpth' + str(depth) + '_colsPerTree' + str(colsample_bytree) + '.csv')\n",
    "    my_mojo = model.download_mojo(path = save_to_path + '/XGboost/', get_genmodel_jar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h2o.init(max_mem_size = h2o_mem, nthreads = h2o_threads)\n",
    "\n",
    "datasets = os.listdir(target_test_path)\n",
    "for file_name in datasets:\n",
    "    h2o.remove_all()\n",
    "    if algorithm == 'GLM':\n",
    "        GLM_gTruth_oneByOne(file_name, target_test_path, save_to_path, combined_preds[0], combined_preds[1])\n",
    "    elif algorithm == 'RF':\n",
    "        RF_gTruth_oneByOne(file_name, target_test_path, save_to_path, combined_preds[0], combined_preds[1])\n",
    "    elif algorithm == 'XGboost':\n",
    "        XGboost_gTruth_oneByOne(file_name, target_test_path, save_to_path, combined_preds[0], combined_preds[1], combined_preds[2])\n",
    "    h2o.remove_all()\n",
    "    h2o.remove_all()\n",
    "\n",
    "h2o.remove_all()\n",
    "h2o.cluster().shutdown()\n",
    "\n",
    "#Calculate method time for the test dataset.\n",
    "datasets = os.listdir(save_to_path + '/' + algorithm)\n",
    "for name in datasets:\n",
    "    if name.split('.')[-1] == \"csv\":\n",
    "        df = pd.read_csv(save_to_path + '/' + algorithm + '/' + name, header = 0, sep = ',')\n",
    "        train_time = df.iloc[0,-1]\n",
    "        rmse = df.iloc[0,-2]\n",
    "    \n",
    "method_time = mfe_time + pred_time + train_time\n",
    "print('Mfe_time:', mfe_time)\n",
    "print('Prediction_time:', pred_time)\n",
    "print('Train_time:', train_time)\n",
    "print('\\n>>> Total_method_time:' + str(method_time)+'s <<<')\n",
    "print('\\n+++ method rmse:'+ str(rmse) + ' +++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

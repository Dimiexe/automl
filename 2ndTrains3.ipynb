{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.optimizers as opts\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "#Paths to access the necessary data\n",
    "path_to_dataset_folder = 'C:/Users/Dimiexe/Desktop/DATAsets'\n",
    "tree_meta_feature_path = path_to_dataset_folder + '/DATA/tree_metafeatures_for_test_CSVs'\n",
    "ground_truth_path = path_to_dataset_folder + '/DATA/ground_truth'\n",
    "metaModel_data_path = path_to_dataset_folder + '/DATA/metaModel_Datasets'\n",
    "regularization_path = path_to_dataset_folder + '/regularization'\n",
    "normalization_path = path_to_dataset_folder + '/normalization'\n",
    "model_path = path_to_dataset_folder + '/final_models'\n",
    "manual_meta_feature_path = path_to_dataset_folder + '/DATA/manual_metafeatures_for_test_CSVs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines the extracted metafeatures and ground truth for each dataset into a new dataset.\n",
    "#Every line contains the 16 metafeatures + 2 (3 for xgboost) ground truth hyperparameters and represents\n",
    "#a different dataset. Returns that unified dataset.\n",
    "def build_dataSet_DL(tree_meta_feature_path, ground_truth_path, manual_meta_feature_path = None,\n",
    "                     algo = 'RF', method = 'drop', eco = True\n",
    "                    ):\n",
    "    for i,f in enumerate(os.listdir(tree_meta_feature_path + '/' + method)):\n",
    "        line = pd.read_csv(tree_meta_feature_path + '/' + method + '/' + f,\n",
    "                           names = ['TreeDiam', 'TreeHeight', 'TotalNodes', 'TotalLeaves',\n",
    "                                    'maxNodePerLevel', 'meanNodePerLevel', 'stdNodePerLevel',\n",
    "                                    'ShortBranch', 'meanBranch', 'stdBranch','maxFeatureFreq',\n",
    "                                    'minFeatureFreq', 'meanFeatureFreq', 'stdFeatureFreq','NaNsPerLine'\n",
    "                                   ],\n",
    "                           header = None, sep=',')\n",
    "        if not manual_meta_feature_path == None:\n",
    "            line2 = pd.read_csv(manual_meta_feature_path + '/' + method + '/' + f,\n",
    "                                names = ['nr_inst', 'nr_attr', 'inst_to_attr', 'nr_cat', 'nr_num', 'cat_to_num',\n",
    "                                         'nr_class_mean', 'nr_class_std', 'cor_mean', 'cor_std', 'cov_mean', 'cov_std',\n",
    "                                         'kurtosis_mean', 'kurtosis_std', 'mad_mean', 'mad_std', 'max_mean', 'max_std',\n",
    "                                         'mean_mean', 'mean_std', 'median_mean', 'median_std', 'min_mean', 'min_std',\n",
    "                                         'std_mean', 'std_std', 'skew_mean', 'skew_std','var_mean', 'var_std'\n",
    "                                        ],\n",
    "                                header = None, sep=',')\n",
    "            line = pd.concat([line, line2], ignore_index = False, axis = 1)\n",
    "        line2 = pd.read_csv(ground_truth_path + '/' + algo + '/' + f, header = 0, sep=',')\n",
    "        if algo == 'GLM':\n",
    "            line2.drop(line2.columns[2:], axis = 1, inplace = True)\n",
    "        elif algo == 'RF':\n",
    "            if eco:\n",
    "                line2.drop(line2.columns[:3], axis = 1, inplace = True)\n",
    "            line2.drop(line2.columns[2:], axis = 1, inplace = True)                \n",
    "        elif algo == 'XGboost':\n",
    "            if eco:\n",
    "                line2.drop(line2.columns[:4], axis = 1, inplace = True)\n",
    "            line2.drop(line2.columns[3:], axis = 1, inplace = True)\n",
    "        else:\n",
    "            print(\"\\n>>> Algorithm '\" + algo + \"' not valid. <<<\")\n",
    "            return None\n",
    "        line = pd.concat([line, line2], ignore_index = False, axis = 1)\n",
    "        if i==0:\n",
    "            df = line\n",
    "        else:\n",
    "            df = pd.concat([df, line], ignore_index = True, axis = 0)\n",
    "    df.to_csv(metaModel_data_path + '/' + algo + '_' + method + '.csv', sep =',', index = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_dataset(data, algo, method, regularize_y, regularization_path):\n",
    "    regularization_means = pd.read_csv(regularization_path + '/regularization_means_' + algo + '_' + method + '.csv')\n",
    "    regularization_stds = pd.read_csv(regularization_path + '/regularization_stds_' + algo + '_' + method + '.csv')\n",
    "    if regularize_y:\n",
    "        L = data.columns\n",
    "    else:\n",
    "        if algo in ['RF', 'GLM']:\n",
    "            L = data.columns[0:-2]\n",
    "        elif algo == 'XGboost':\n",
    "            L = data.columns[0:-3]\n",
    "    for j in L:\n",
    "        for i,x in enumerate(data.loc[:,j]):\n",
    "            data.loc[i,j] = (x-regularization_means.loc[0,j])/regularization_stds.loc[0,j]\n",
    "    return data\n",
    "\n",
    "def normalize_dataset(data, algo, method, normalize_y, normalization_path):\n",
    "    normal_mins = pd.read_csv(normalization_path + '/normalizaton_mins_' + algo + '_' + method + '.csv')\n",
    "    normal_maxes = pd.read_csv(normalization_path + '/normalizaton_maxes_' + algo + '_' + method + '.csv')\n",
    "    if normalize_y:\n",
    "        L = data.columns\n",
    "    else:\n",
    "        if algo in ['RF', 'GLM']:\n",
    "            L = data.columns[0:-2]\n",
    "        elif algo == 'XGboost':\n",
    "            L = data.columns[0:-3]\n",
    "    for j in L:\n",
    "        for i,x in enumerate(data.loc[:,j]):\n",
    "            data.loc[i,j] = ( x-normal_mins.loc[0,j] ) / (normal_maxes.loc[0,j] - normal_mins.loc[0,j])\n",
    "    return data\n",
    "\n",
    "def deregularize_preds(preds, algo, method, regularization_path):\n",
    "    regularization_means = pd.read_csv(regularization_path + '/regularization_means_' + algo + '_' + method + '.csv')\n",
    "    regularization_stds = pd.read_csv(regularization_path + '/regularization_stds_' + algo + '_' + method + '.csv')\n",
    "    if algo in ['RF','GLM']:\n",
    "        L = [-1,-2]\n",
    "    elif algo == 'XGboost':\n",
    "        L = [-1,-2,-3]\n",
    "    for j in L:\n",
    "        for i,x in enumerate(preds[:,j]):\n",
    "            preds[i,j] = (x * regularization_stds.iloc[0,j] + regularization_means.iloc[0,j])\n",
    "    return preds\n",
    "\n",
    "def denormalize_preds(preds, algo, method, normalization_path):    \n",
    "    normal_mins = pd.read_csv(normalization_path + '/normalizaton_mins_' + algo + '_' + method + '.csv')\n",
    "    normal_maxes = pd.read_csv(normalization_path + '/normalizaton_maxes_' + algo + '_' + method + '.csv')\n",
    "    if algo in ['RF','GLM']:\n",
    "        L = [-1,-2]\n",
    "    elif algo == 'XGboost':\n",
    "        L = [-1,-2,-3]\n",
    "    for j in L:\n",
    "        for i,x in enumerate(preds[:,j]):\n",
    "            preds[i,j] = x * (normal_maxes.iloc[0,j] - normal_mins.iloc[0,j]) + normal_mins.iloc[0,j]\n",
    "    return preds\n",
    "\n",
    "# Standardize data and splits to predictors (x) and targets (y)\n",
    "# Standardization options:\n",
    "# 'regularize' for regularization (mean = 0, std = 1)\n",
    "# 'normalize' for normalization (range of values in [0,1])\n",
    "def Data_prep(data, algo, method, regularization_path, normalization_path, standardize = 'regularize', regularize_y = False, normalize_y = False):\n",
    "    if standardize == 'regularize':\n",
    "        data = regularize_dataset(data, algo, method, regularize_y, regularization_path)\n",
    "    elif standardize == 'normalize':\n",
    "        data = normalize_dataset(data, algo, method, normalize_y, normalization_path)\n",
    "    else:\n",
    "        print('No regularization performed. Results will be sub-optimal.')\n",
    "    data = data.values\n",
    "    if algo in ['RF', 'GLM']:\n",
    "        y = data[:, -2:]\n",
    "        x = data[:, 0:-2]\n",
    "    elif algo == 'XGboost':\n",
    "        y = data[:, -3:]\n",
    "        x = data[:, 0:-3]\n",
    "    else:\n",
    "        print('\\n>>> Not supported algorithm. Returning None! <<<')\n",
    "        return None\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns \"train\" and \"test\" pandas.DataFrames split acording to the provided ratio.\n",
    "def split_train_test_indices(dataset, split_ratio = .15):\n",
    "    import random\n",
    "    rows = dataset.shape[0]\n",
    "    test_rows = round(rows * split_ratio)\n",
    "    train_rows = rows - test_rows\n",
    "    \n",
    "    #Generate train_data\n",
    "    train_indices = []\n",
    "    while len(train_indices) < train_rows:\n",
    "        candidate = random.randint(0, rows-1)\n",
    "        if candidate not in train_indices:\n",
    "            train_indices.append(candidate)\n",
    "    \n",
    "    #Generate test data\n",
    "    test_indices = []\n",
    "    for j in range(rows):\n",
    "        if j not in train_indices:\n",
    "            test_indices.append(j)    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "def split_train_test(dataset, split_ratio = .15):\n",
    "    import random\n",
    "    rows = len(dataset)\n",
    "    test_rows = round(rows * split_ratio)\n",
    "    train_rows = rows - test_rows\n",
    "    \n",
    "    #Generate train_data\n",
    "    train_indices = []\n",
    "    while len(train_indices) < train_rows:\n",
    "        candidate = random.randint(0, rows-1)\n",
    "        if candidate not in train_indices:\n",
    "            train_indices.append(candidate)\n",
    "    train = dataset.loc[train_indices,:]\n",
    "    train.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    #Generate test data\n",
    "    test_indices = []\n",
    "    for j in range(rows):\n",
    "        if j not in train_indices:\n",
    "            test_indices.append(j)\n",
    "    test = dataset.loc[test_indices,:]\n",
    "    test.reset_index(drop=True, inplace = True)    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains and returns the deep learning estimator for the specified algorithm.\n",
    "#The below code is based on the code found at:\n",
    "#https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "#and is tweked acording to the needs of this project.\n",
    "\n",
    "# define function to make base model\n",
    "def create_model(input_dim = 15, algo = 'GLM', hidden = 5, two_layers = False, half_on_second = False):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    if two_layers:\n",
    "        if half_on_second:\n",
    "            model.add(Dense(round(hidden/2), kernel_initializer='normal'))\n",
    "        else:\n",
    "            model.add(Dense(hidden, kernel_initializer='normal'))\n",
    "    if algo in ['GLM', 'RF']:\n",
    "        model.add(Dense(2, kernel_initializer='normal'))\n",
    "    elif algo == 'XGboost':\n",
    "        model.add(Dense(3, kernel_initializer='normal'))\n",
    "    else:\n",
    "        print(\"\\n>>> Algorithm '\" + algo + \"' not valid. <<<\")\n",
    "        return None\n",
    "    # Compile model\n",
    "    model.compile(loss = 'mse', optimizer = opts.Adam(learning_rate=0.01))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_preds_calcs(param_vector, single_pred):\n",
    "    param_vector2 = [abs(x - single_pred) for x in param_vector]\n",
    "    temp = 0   #Store parameter value\n",
    "    ind = 100000  #Store parameter comparison value\n",
    "    for i,pa in enumerate(param_vector2):\n",
    "        if pa < ind:\n",
    "            ind = pa\n",
    "            temp = param_vector[i]\n",
    "    return temp\n",
    "\n",
    "def fix_preds_NN(preds, algo, lambdas):\n",
    "    in_preds = pd.DataFrame({})\n",
    "    length = len(preds)\n",
    "    temp0 = []\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    if algo == 'GLM':\n",
    "        y = range(0, 1100, 125)\n",
    "        param_alpha = [x / 1000 for x in y]\n",
    "        param_lambda = set(lambdas)\n",
    "        param_lambda = list(param_lambda)  #Create list of unique train lambda values\n",
    "        for i in range(length):\n",
    "            temp0.append(fix_preds_calcs(param_alpha, preds[i,0]))\n",
    "            temp1.append(fix_preds_calcs(param_lambda, preds[i,1]))\n",
    "        in_preds['A'] = temp0\n",
    "        in_preds['B'] = temp1\n",
    "                \n",
    "    elif algo =='RF':\n",
    "        param_tree = [25,50,75,100,200,300,400,500]\n",
    "        param_depth = [20,40,60,80]\n",
    "        for i in range(length):\n",
    "            temp0.append(fix_preds_calcs(param_tree, preds[i,0]))\n",
    "            temp1.append(fix_preds_calcs(param_depth, preds[i,1]))\n",
    "        in_preds['A'] = temp0\n",
    "        in_preds['B'] = temp1\n",
    "        \n",
    "    elif algo == 'XGboost':\n",
    "        for i in range(length):\n",
    "            param_tree = [25,50,100,200]\n",
    "            param_depth = [6,10,15]\n",
    "            colsample_bytree = [.6,.7,.8,.9]\n",
    "            temp0.append(fix_preds_calcs(param_tree, preds[i,0]))\n",
    "            temp1.append(fix_preds_calcs(param_depth, preds[i,1]))\n",
    "            temp2.append(fix_preds_calcs(colsample_bytree, preds[i,2]))\n",
    "        in_preds['A'] = temp0\n",
    "        in_preds['B'] = temp1\n",
    "        in_preds['C'] = temp2\n",
    "    return in_preds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# This is a sample of a scheduler I used in the past\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.85\n",
    "    decay_step = 300\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * pow(decay_rate, np.floor(epoch / decay_step))\n",
    "    return lr\n",
    "\n",
    "lr = LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Run >>> ONCE <<< to extract regularization statistics,\n",
    "#and build meta-learning datasets.\n",
    "\n",
    "def extract_normalization_info(data, algo, method, regularization_path, normalization_path):\n",
    "    mean_dict = {}\n",
    "    std_dict = {}\n",
    "    min_dict = {}\n",
    "    max_dict = {}\n",
    "    for i,c in enumerate(data.columns):\n",
    "        meanV = data.iloc[:,i].mean()\n",
    "        stdV = data.iloc[:,i].std()\n",
    "        minV = data.iloc[:,i].min()\n",
    "        maxV = data.iloc[:,i].max()\n",
    "        mean_dict[c] = [meanV]\n",
    "        std_dict[c] = [stdV]\n",
    "        min_dict[c] = [minV]\n",
    "        max_dict[c] = [maxV]\n",
    "    temp = pd.DataFrame(mean_dict)\n",
    "    temp.to_csv(regularization_path + '/regularization_means_' + algo + '_' + method + '.csv', index =False)\n",
    "    temp = pd.DataFrame(std_dict)\n",
    "    temp.to_csv(regularization_path + '/regularization_stds_' + algo + '_' + method + '.csv', index =False)\n",
    "    temp = pd.DataFrame(min_dict)\n",
    "    temp.to_csv(normalization_path + '/normalizaton_mins_' + algo + '_' + method + '.csv', index =False)\n",
    "    temp = pd.DataFrame(max_dict)\n",
    "    temp.to_csv(normalization_path + '/normalizaton_maxes_' + algo + '_' + method + '.csv', index =False)\n",
    "        \n",
    "for algo in ['RF','GLM','XGboost']:\n",
    "    for method in ['drop', 'mean', 'median']:\n",
    "        dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                   ground_truth_path,\n",
    "                                   manual_meta_feature_path,\n",
    "                                   algo,\n",
    "                                   method,\n",
    "                                   eco = True\n",
    "                                  )\n",
    "        extract_normalization_info(dataset,\n",
    "                                   algo,\n",
    "                                   method,\n",
    "                                   regularization_path,\n",
    "                                   normalization_path\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------\n",
    "# THIS WAS USED TO EXPERIMENT FOR THE NUMBER OF NEURONS IN THE HIDDEN LAYER\n",
    "#              >>> NOT TO BE USED ANYMORE <<<\n",
    "# USE THE FINAL SEGMENT BELOW FOR TRAINING THE FINAL MODELS\n",
    "#--------------------------------------------------------------------------\n",
    "#Create all datasets for the Neural Networks.\n",
    "#3 datasets are generated per algorithm, 1 for each NaN imputation method (drop,mean,median).\n",
    "kfold = KFold(n_splits=10)\n",
    "two_layers = False\n",
    "meta = 'tree'\n",
    "es = EarlyStopping(monitor='loss', mode='min', patience = 50, verbose = 0)\n",
    "for standardize in  ['regularize', 'normalize']:\n",
    "    for standardize_y in [False, True]:\n",
    "        if standardize == 'regularize':\n",
    "            if not standardize_y:\n",
    "                print('################## Experiment 1 #####################')\n",
    "                print('metafeature set:',meta)\n",
    "                print('standardize: regularize')\n",
    "                print('regularize_y: False')\n",
    "            else:\n",
    "                print('################## Experiment 2 #####################')\n",
    "                print('metafeature set:',meta)\n",
    "                print('standardize: regularize')\n",
    "                print('regularize_y: True')\n",
    "        elif standardize == 'normalize':\n",
    "            if not standardize_y:\n",
    "                print('################## Experiment 3 #####################')\n",
    "                print('metafeature set:',meta)\n",
    "                print('standardize: normalize')\n",
    "                print('normalize_y: False')\n",
    "            else:\n",
    "                print('################## Experiment 4 #####################')\n",
    "                print('metafeature set:',meta)\n",
    "                print('standardize: normalize')\n",
    "                print('normalize_y: True')\n",
    "        #print('two_layers =',two_layers)\n",
    "\n",
    "        for algo in ['RF','XGboost','GLM']:\n",
    "            print('------ Algo = ' + algo + ' ------')\n",
    "            for method in ['drop', 'mean', 'median']:\n",
    "                print('NaN method: ' + method)\n",
    "                if meta == 'tree':\n",
    "                    input_dim = 15\n",
    "                    dataset = build_dataSet_DL(tree_meta_feature_path, ground_truth_path,\n",
    "                                               algo = algo, method = method, eco = True\n",
    "                                              )\n",
    "                elif meta == 'all':\n",
    "                    input_dim = 45\n",
    "                    dataset = build_dataSet_DL(tree_meta_feature_path, ground_truth_path,\n",
    "                                               manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                               algo = algo, method = method, eco = True\n",
    "                                              )\n",
    "                x, y = Data_prep(dataset, algo, method,\n",
    "                                 regularization_path, normalization_path,\n",
    "                                 standardize = standardize,\n",
    "                                 regularize_y = standardize_y,\n",
    "                                 normalize_y = standardize_y\n",
    "                                )\n",
    "                for hidden in [1,2,3,4,5]:\n",
    "                    print(str(hidden) + ' hidden neurons')\n",
    "                    temp =[]\n",
    "                    for i in range(10):\n",
    "                        estimator = KerasRegressor(build_fn = create_model,\n",
    "                                                   input_dim = input_dim,\n",
    "                                                   algo = algo,\n",
    "                                                   hidden = hidden,\n",
    "                                                   two_layers = two_layers\n",
    "                                                  )\n",
    "                        results = cross_val_score(estimator, X = x, y = y,\n",
    "                                                  fit_params = {#'callbacks':[es],\n",
    "                                                                'epochs':200,\n",
    "                                                                'verbose':0,\n",
    "                                                                'batch_size':5\n",
    "                                                               },\n",
    "                                                  cv = kfold,\n",
    "                                                  scoring = 'neg_root_mean_squared_error')\n",
    "                        #print(results)\n",
    "                        temp = np.concatenate((temp,results))\n",
    "                    print(\"Results: %.10f RMSE, %.10f std\" % (abs(temp.mean()), temp.std()))\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------\n",
    "#                             VERSION 2\n",
    "# THIS WAS USED TO EXPERIMENT FOR THE NUMBER OF NEURONS IN THE HIDDEN LAYER\n",
    "#              >>> NOT TO BE USED ANYMORE <<<\n",
    "# USE THE FINAL SEGMENT BELOW FOR TRAINING THE FINAL MODELS\n",
    "#                             VERSION 2\n",
    "#--------------------------------------------------------------------------\n",
    "#Create all datasets for the Neural Networks.\n",
    "#3 datasets are generated per algorithm, 1 for each NaN imputation method (drop,mean,median).\n",
    "kfold = KFold(n_splits=10)\n",
    "standardize = 'normalize'\n",
    "exp_i = 0\n",
    "for standardize_y in [False, True]: #normalize_y\n",
    "    for meta in ['tree', 'all']:\n",
    "        if meta == 'tree':\n",
    "            input_dim = 15\n",
    "        else:\n",
    "            input_dim = 45\n",
    "        for two_layers in [False,True]:\n",
    "            for hos in [False,True]:\n",
    "                if two_layers:\n",
    "                    exp_i+=1\n",
    "                    print('################### Experiment ' + str(exp_i) + ' ###################')\n",
    "                    print('standardize method:',standardize)\n",
    "                    print('standardize_y:',standardize_y)\n",
    "                    print('metafeature set:',meta)\n",
    "                    print('two_layers =',two_layers)\n",
    "                    if two_layers:\n",
    "                        print('second layer has half neurons:',hos)\n",
    "\n",
    "                    for algo in ['GLM', 'RF','XGboost']:\n",
    "                        print('------ Algo = ' + algo + ' ------')\n",
    "                        for method in ['drop', 'mean', 'median']:\n",
    "                            print('NaN method: ' + method)\n",
    "                            #Prepare the data\n",
    "                            if meta == 'all':        \n",
    "                                dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                                           ground_truth_path,\n",
    "                                                           manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                                           algo = algo, method = method\n",
    "                                                          )\n",
    "                            elif meta == 'tree':\n",
    "                                dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                                           ground_truth_path,\n",
    "                                                           algo = algo, method = method\n",
    "                                                          )\n",
    "                            x, y = Data_prep(dataset, algo, method,\n",
    "                                             regularization_path, normalization_path,\n",
    "                                             standardize = standardize,\n",
    "                                             regularize_y = standardize_y,\n",
    "                                             normalize_y = standardize_y\n",
    "                                            )\n",
    "                            for hidden in [5,10,20]:\n",
    "                                print(str(hidden) + ' hidden neurons')\n",
    "                                temp =[]\n",
    "                                for i in range(20):\n",
    "                                    estimator = KerasRegressor(build_fn = create_model,\n",
    "                                                               input_dim = input_dim,\n",
    "                                                               algo = algo,\n",
    "                                                               hidden = hidden,\n",
    "                                                               two_layers = two_layers,\n",
    "                                                               half_on_second = hos\n",
    "                                                              )\n",
    "                                    results = cross_val_score(estimator, X = x, y = y,\n",
    "                                                              fit_params = {'epochs':100,\n",
    "                                                                            'verbose':0,\n",
    "                                                                            'batch_size':5\n",
    "                                                                           },\n",
    "                                                              cv = kfold,\n",
    "                                                              scoring = 'neg_root_mean_squared_error'\n",
    "                                                             )\n",
    "                                    #print(results)\n",
    "                                    temp = np.concatenate((temp,results))\n",
    "                                print(\"Results: %.10f RMSE, %.10f std\" % (abs(temp.mean()), temp.std()))\n",
    "                    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this once for each cross validation AND prediction session, as the row\n",
    "#indices are stored in \"train_indices\" and \"test_indices\" and are the same\n",
    "#for both crossvalidation and fit processes.\n",
    "\n",
    "manual_indices = True\n",
    "\n",
    "split_ratio = .15 #Training/Testing set ratio.\n",
    "kfold = KFold(n_splits=10)\n",
    "#Creating the dataset once to get its dimensions. We need the number of rows, not columns,\n",
    "#so any \"algo\" is aplicable. We dont need the values of the rows, just the rows, so any\n",
    "#\"method\" is also aplicable as is the option for eco_best, or absolute_best.\n",
    "#Also we dont care about the number of metafeatures contained in the dataset, as they\n",
    "#make up the columns not the rows.\n",
    "dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                           ground_truth_path,\n",
    "                           algo = 'RF', method = 'drop', eco = True)\n",
    "#Get the indices for test and train instances based on extracted dataset dimensions.\n",
    "if manual_indices:\n",
    "    train_indices = [28,58,31,29,20,147,157,155,62,159,137,112,118,134,164,90,142,19,60,161,94,18,84,67,56,61,74,30,52,2,117,1,126,65,70,163,73,114,27,139,63,89,154,166,46,6,37,116,8,173,140,13,122,39,135,174,14,36,24,104,124,130,16,141,7,149,167,79,43,69,53,9,77,12,98,22,165,93,85,11,100,108,3,4,145,123,144,10,119,105,129,92,80,33,71,99,107,25,148,97,106,42,109,82,158,51,152,50,168,40,0,146,86,153,41,156,170,136,35,57,143,44,83,102,54,26,177,131,169,120,138,81,68,133,175,132,32,45,78,76,48,59,15,88,113,64,96,171,128,66,75]\n",
    "    test_indices = [5,17,21,23,34,38,47,49,55,72,87,91,95,101,103,110,111,115,121,125,127,150,151,160,162,172,176]\n",
    "else:\n",
    "    train_indices, test_indices = split_train_test_indices(dataset, split_ratio = .15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histories = {}\n",
    "session = 'fit_pred' #'cv'\n",
    "plot_true_preds = True\n",
    "standardize = 'regularize'\n",
    "hidden = 5\n",
    "exp_counter = 0\n",
    "for meta in ['tree', 'all']:\n",
    "    for standardize_y in [False, True]:\n",
    "        for algo in ['GLM', 'RF', 'XGboost']:\n",
    "            exp_counter += 1\n",
    "            #The bellow if statements are setting the optimal paramenters for the meta-model\n",
    "            #according to the case in question.\n",
    "            #----- Using ONLY tree metafeatures -----\n",
    "            if meta == 'tree':\n",
    "                if standardize_y:\n",
    "                    if algo == 'GLM':\n",
    "                        two_layers = True\n",
    "                        half_on_second = False\n",
    "                        method = 'drop'\n",
    "                    elif algo == 'RF':\n",
    "                        two_layers = True\n",
    "                        half_on_second = True\n",
    "                        method = 'drop'\n",
    "                    elif algo =='XGboost':\n",
    "                        two_layers = True\n",
    "                        half_on_second = True\n",
    "                        method = 'mean'\n",
    "                else:\n",
    "                    if algo == 'GLM':\n",
    "                        two_layers = False\n",
    "                        half_on_second = None\n",
    "                        method = 'drop'\n",
    "                    elif algo == 'RF':\n",
    "                        two_layers = True\n",
    "                        half_on_second = False\n",
    "                        method = 'drop'\n",
    "                    elif algo =='XGboost':\n",
    "                        two_layers = False\n",
    "                        half_on_second = None\n",
    "                        method = 'mean'\n",
    "            #----- Using BOTH tree + manual metafeatures -----\n",
    "            elif meta =='all':\n",
    "                if standardize_y:\n",
    "                    if algo == 'GLM':\n",
    "                        two_layers = True\n",
    "                        half_on_second = True\n",
    "                        method = 'drop'\n",
    "                    elif algo == 'RF':\n",
    "                        two_layers = True\n",
    "                        half_on_second = True\n",
    "                        method = 'drop'\n",
    "                    elif algo =='XGboost':\n",
    "                        two_layers = True\n",
    "                        half_on_second = True\n",
    "                        method = 'mean'\n",
    "                else:\n",
    "                    if algo == 'GLM':\n",
    "                        two_layers = False\n",
    "                        half_on_second = None\n",
    "                        method = 'drop'\n",
    "                    elif algo == 'RF':\n",
    "                        two_layers = False\n",
    "                        half_on_second = None\n",
    "                        method = 'drop'\n",
    "                    elif algo =='XGboost':\n",
    "                        two_layers = False\n",
    "                        half_on_second = None\n",
    "                        method = 'mean'\n",
    "            #Printing each experiment's configuration\n",
    "            print('------ Experiment ' + str(exp_counter) + ' ------')\n",
    "            print('Metafeature set:',meta)\n",
    "            print('Standardization:',standardize)\n",
    "            if standardize == 'regularize':\n",
    "                print('regularize_y:',standardize_y)\n",
    "            elif standardize == 'normalize':\n",
    "                print('normalize_y:',standardize_y)\n",
    "            print('Hidden neurons:', hidden)\n",
    "            print('2nd hidden layer:',two_layers)\n",
    "            if two_layers:\n",
    "                print('2nd hidden has half neurons:',half_on_second)\n",
    "            print('Algo:',algo)\n",
    "            print('Nan method:',method)\n",
    "    \n",
    "            #Prepare the data\n",
    "            if meta == 'all':\n",
    "                input_dim = 45\n",
    "                dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                           ground_truth_path,\n",
    "                                           manual_meta_feature_path = manual_meta_feature_path,\n",
    "                                           algo = algo, method = method\n",
    "                                          )\n",
    "            elif meta == 'tree':\n",
    "                input_dim = 15\n",
    "                dataset = build_dataSet_DL(tree_meta_feature_path,\n",
    "                                           ground_truth_path,\n",
    "                                           algo = algo, method = method\n",
    "                                          )\n",
    "            #Creating train and test datasets according to predefined row indices.\n",
    "            train = dataset.loc[train_indices,:]\n",
    "            train.reset_index(drop=True, inplace = True)\n",
    "            test = dataset.loc[test_indices,:]\n",
    "            test.reset_index(drop=True, inplace = True)\n",
    "            x_train, y_train = Data_prep(train, algo, method,\n",
    "                                         regularization_path, normalization_path,\n",
    "                                         standardize = standardize,\n",
    "                                         regularize_y = standardize_y,\n",
    "                                         normalize_y = standardize_y\n",
    "                                        )\n",
    "            x_test, y_test = Data_prep(test, algo, method,\n",
    "                                       regularization_path, normalization_path,\n",
    "                                       standardize = standardize)\n",
    "    \n",
    "            #Creating the model\n",
    "            model = create_model(input_dim = input_dim, algo = algo, hidden = hidden,\n",
    "                                 two_layers = two_layers, half_on_second = half_on_second\n",
    "                                )\n",
    "    \n",
    "            #If doing crossvalidation session\n",
    "            if session == 'cv':\n",
    "                history = model.fit(x = x_train, y = y_train,\n",
    "                                    validation_split = 0.15,\n",
    "                                    epochs = 2000,\n",
    "                                    batch_size = 5,\n",
    "                                    verbose = 0\n",
    "                                   )\n",
    "                histories[exp_counter-1] = history\n",
    "                plt.plot(np.sqrt(history.history['loss']))\n",
    "                plt.plot(np.sqrt(history.history['val_loss']))\n",
    "                plt.title('model loss: rmse')\n",
    "                plt.ylabel('loss')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'validation'], loc='upper left')\n",
    "                plt.show()\n",
    "        \n",
    "            #If doing fit and predictions session\n",
    "            elif session == 'fit_pred':\n",
    "                es = EarlyStopping(monitor='val_loss',\n",
    "                                   mode='min',\n",
    "                                   patience = 500,\n",
    "                                   verbose = 1\n",
    "                                  )\n",
    "                mc = ModelCheckpoint(model_path + '/best_model_' + algo + '_' + method +'.h5',\n",
    "                                     monitor='val_loss',\n",
    "                                     mode='min',\n",
    "                                     verbose=0,\n",
    "                                     save_best_only=True\n",
    "                                    )\n",
    "                history = model.fit(x = x_train, y = y_train,\n",
    "                                    validation_split = 0.15,\n",
    "                                    callbacks = [es, mc],\n",
    "                                    epochs = 2000,\n",
    "                                    batch_size = 5,\n",
    "                                    verbose = 0)\n",
    "                model = load_model(model_path + '/best_model_' + algo + '_' + method +'.h5')\n",
    "                predictions = model.predict(x_test, verbose = 0)\n",
    "                if standardize == 'regularize' and standardize_y:\n",
    "                    predictions = deregularize_preds(predictions, algo, method, regularization_path)\n",
    "                elif standardize == 'normalize' and standardize_y:        \n",
    "                    predictions = denormalize_preds(predictions, algo, method, normalization_path)\n",
    "                y_cols = predictions.shape[1]\n",
    "                fig,a =  plt.subplots(1,y_cols,figsize=(20, 5))\n",
    "                x = range(predictions.shape[0])\n",
    "        \n",
    "                y_lambda_values = []\n",
    "                if algo == 'GLM':\n",
    "                    y_lambda_values = np.concatenate([y_train[:,1],y_test[:,1]], axis=0)\n",
    "                preds_fixed = fix_preds(predictions, algo, y_lambda_values)\n",
    "                temp_rmse = []\n",
    "                for target_column in range(y_cols):\n",
    "                    #Plot ground truth\n",
    "                    a[target_column].plot(x,y_test[:,target_column],'bo')\n",
    "                    #Plotting quantized predictions\n",
    "                    a[target_column].plot(x,preds_fixed[:,target_column],'yx')\n",
    "                    if plot_true_preds:\n",
    "                        a[target_column].plot(x,predictions[:,target_column],'r+')\n",
    "                        a[target_column].legend(['ground truth', 'quantized preds', 'real preds'], loc='upper left')\n",
    "                    else:\n",
    "                        a[target_column].legend(['ground truth', 'quantized preds'], loc='upper left')                        \n",
    "    \n",
    "                    #Calculate rmse separately for each parameter\n",
    "                    temp2 = math.sqrt(np.mean(np.square(preds_fixed[:,target_column] - y_test[:,target_column])))\n",
    "                    print('RMSE for paramenter ' + str(target_column+1) + ': '+ str(round(10000*temp2)/10000) + ' scale is ' + str(round(10000*temp2/np.nanmax(y_test[:,target_column]))/100) + '%')\n",
    "                plt.show()\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Run to zoom in at the cross validation learning curves\n",
    "for i in range(len(histories)):\n",
    "    plt.plot(np.sqrt(histories[i].history['loss'])[0:500])\n",
    "    plt.plot(np.sqrt(histories[i].history['val_loss'])[0:500])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to save test and train indices\n",
    "with open(path_to_dataset_folder + '/train_indices.csv', 'w') as f:\n",
    "    for i,temp in enumerate(train_indices):\n",
    "        if i < len(train_indices)-1:\n",
    "            f.write(str(temp) + ',')\n",
    "        else:\n",
    "            f.write(str(temp))\n",
    "            \n",
    "with open(path_to_dataset_folder + '/test_indices.csv', 'w') as f:\n",
    "    for i,temp in enumerate(test_indices):\n",
    "        if i < len(test_indices)-1:\n",
    "            f.write(str(temp) + ',')\n",
    "        else:\n",
    "            f.write(str(temp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
